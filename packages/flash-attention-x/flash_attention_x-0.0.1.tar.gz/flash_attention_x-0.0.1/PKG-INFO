Metadata-Version: 2.1
Name: flash_attention_x
Version: 0.0.1
Summary: A flash attention(s) implementation in triton.
Home-page: https://github.com/ahxt/flash-attention-x
Author: Xiaotian Han
Author-email: your.email@example.com
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Requires-Python: >=3.8
Description-Content-Type: text/markdown

# Flash Attention X

Flash Attention X is a flash attention(s) implementation using Triton, including flash_attention_full, flash_attention_causal and flash_attention_bias...

## Installation

You can install Flash Attention X using pip:
```bash
pip install flash_attention_x
```

```bash
pip install -e .
```


## Requirements

- Python >= 3.8
- Triton >= 2.3.0

The package is tested with Triton 2.3.0 and CUDA 12.0.

## Features

- Efficient implementation of flash attention(s), including flash_attention_full, flash_attention_causal and flash_attention_bias...
- Built using Triton for optimized performance

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Author

Xiaotian Han
