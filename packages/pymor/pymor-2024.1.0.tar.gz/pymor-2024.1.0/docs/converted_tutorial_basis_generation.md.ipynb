{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66a0c80f",
   "metadata": {},
   "source": [
    "```{try_on_binder}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1b1e4c",
   "metadata": {
    "load": "myst_code_init.py",
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import matplotlib as mpl\n",
    "from IPython import get_ipython\n",
    "\n",
    "import pymor.tools.random\n",
    "\n",
    "ip = get_ipython()\n",
    "if ip is not None:\n",
    "    ip.run_line_magic('matplotlib', 'inline')\n",
    "\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='torch')\n",
    "\n",
    "pymor.tools.random._default_random_state = None\n",
    "\n",
    "mpl.rcParams['figure.facecolor'] = (1.0, 1.0, 1.0, 0.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee479f53",
   "metadata": {},
   "source": [
    "# Tutorial: Building a Reduced Basis\n",
    "\n",
    "In this tutorial we will learn more about {{ VectorArrays }} and how to\n",
    "construct a reduced basis using pyMOR.\n",
    "\n",
    "A reduced basis spans a low dimensional subspace of a {{ Model }}'s\n",
    "{attr}`~pymor.models.interface.Model.solution_space`, in which the\n",
    "{meth}`solutions <pymor.models.interface.Model.solve>` of the {{ Model }}\n",
    "can be well approximated for all {{ parameter_values }}. In this context,\n",
    "time is treated as an additional parameter. So for time-dependent problems,\n",
    "the reduced space (the span of the reduced basis) should approximate the\n",
    "solution for all {{ parameter_values }} and time instances.\n",
    "\n",
    "An upper bound for the possible quality of a reduced space is given by the so-called\n",
    "Kolmogorov {math}`N`-width {math}`d_N(\\mathcal{M})` of the solution manifold\n",
    "\n",
    "```{math}\n",
    "\\mathcal{M} := \\{u(\\mu) \\,|\\, \\mu \\in \\mathcal{P}\\},\n",
    "```\n",
    "\n",
    "given as\n",
    "\n",
    "```{math}\n",
    "d_N(\\mathcal{M}) :=\n",
    "       \\inf_{\\substack{V_N \\subseteq V\\\\ \\operatorname{dim}(V_N) \\leq N}}\\,\n",
    "       \\sup_{x \\in \\mathcal{M}}\\,\n",
    "       \\inf_{v \\in V_N}\\,\n",
    "       \\|x - v\\|.\n",
    "```\n",
    "\n",
    "In these definitions, {math}`V` denotes the\n",
    "{attr}`~pymor.models.interface.Model.solution_space`, {math}`\\mathcal{P} \\subset \\mathbb{R}^p`\n",
    "denotes the set of all {{ parameter_values }} we are interested in, and\n",
    "{math}`u(\\mu)` denotes the {meth}`solution <pymor.models.interface.Model.solve>`\n",
    "of the {{ Model }} for the given {{ parameter_values }} {math}`\\mu`.\n",
    "In pyMOR the set {math}`\\mathcal{P}` is called the {{ ParameterSpace }}.\n",
    "\n",
    "How to read this formula? For each candidate reduced space {math}`V_N` we\n",
    "look at all possible {math}`x \\in \\mathcal{M}` and compute the best-approximation\n",
    "error in {math}`V_N` (the second infimum). The supremum over the infimum\n",
    "is thus the worst-case best-approximation error over all solutions\n",
    "{math}`x = u(\\mu)` of interest.\n",
    "Now we take the infimum of the worst-case best-approximation errors\n",
    "over all possible reduced spaces of dimension at most {math}`N`, and this is\n",
    "{math}`d_N(\\mathcal{M})`.\n",
    "\n",
    "So whatever reduced space of dimension {math}`N` we pick, we\n",
    "will always find a {math}`\\mu` for which the best-approximation error in our\n",
    "space is at least {math}`d_N(\\mathcal{M})`. Reduced basis methods aim at constructing\n",
    "spaces {math}`V_N` for which the worst-case best-approximation error is as\n",
    "close to {math}`d_N(\\mathcal{M})` as possible.\n",
    "\n",
    "However, we will only find a good {math}`V_N` of small dimension\n",
    "{math}`N` if the values {math}`d_N(\\mathcal{M})` decrease quickly for growing\n",
    "{math}`N`. It can be shown that this is the case as soon as {math}`u(\\mu)`\n",
    "analytically depends on {math}`\\mu`, which is true for many problems\n",
    "of interest. More precisely, it can be shown {cite}`BCDDPW11`, {cite}`DPW13` that there are constants\n",
    "{math}`C, c > 0` such that\n",
    "\n",
    "```{math}\n",
    "d_N(\\mathcal{M}) \\leq C \\cdot e^{-N^c}.\n",
    "```\n",
    "\n",
    "In this tutorial we will construct reduced spaces {math}`V_N` for a concrete problem\n",
    "with pyMOR and study their error decay.\n",
    "\n",
    "## Model setup\n",
    "\n",
    "First we need to define a {{ Model }} and a {{ ParameterSpace }} for which we want\n",
    "to build a reduced basis. We choose here the standard\n",
    "{meth}`thermal block <pymor.analyticalproblems.thermalblock.thermal_block_problem>` benchmark\n",
    "problem shipped with pyMOR (see {doc}`getting_started`). However, any pyMOR\n",
    "{{ Model }} can be used, except for Section [Weak Greedy Algorithm](#weak-greedy-algorithm) where\n",
    "some more assumptions have to be made on the {{ Model }}.\n",
    "\n",
    "First we import everything we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c84082",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pymor.basic import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50c064d",
   "metadata": {},
   "source": [
    "Then we build a 3-by-2 thermalblock problem that we discretize using pyMOR's\n",
    "{mod}`built-in discretizers <pymor.discretizers.builtin>` (see\n",
    "{doc}`tutorial_builtin_discretizer` for an introduction to pyMOR's discretization toolkit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed414a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = thermal_block_problem((3,2))\n",
    "fom, _ = discretize_stationary_cg(problem, diameter=1/100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6976559f",
   "metadata": {},
   "source": [
    "Next, we need to define a {{ ParameterSpace }} of {{ parameter_values }} for which\n",
    "the solutions of the full-order model `fom` should be approximated by the reduced basis.\n",
    "We do this by calling the {meth}`~pymor.parameters.base.Parameters.space` method\n",
    "of the |parameters| of `fom`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a099481e",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_space = fom.parameters.space(0.0001, 1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1473a6a4",
   "metadata": {},
   "source": [
    "Here, `0.0001` and `1.` are the common lower and upper bounds for the\n",
    "individual components of all |parameters| of `fom`. In our case `fom`\n",
    "expects a single parameter `'diffusion'` of 6 values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fb481c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fom.parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98aadcaf",
   "metadata": {},
   "source": [
    "If `fom` were to depend on multiple |parameters|, we could also call the\n",
    "{meth}`~pymor.parameters.base.Parameters.space` method with a dictionary\n",
    "of lower and upper bounds per parameter.\n",
    "\n",
    "The main use of {{ ParameterSpaces }} in pyMOR is that they allow to easily sample\n",
    "{{ parameter_values }} from their domain using the methods\n",
    "{meth}`~pymor.parameters.base.ParameterSpace.sample_uniformly` and\n",
    "{meth}`~pymor.parameters.base.ParameterSpace.sample_randomly`.\n",
    "\n",
    "## Computing the snapshot data\n",
    "\n",
    "Reduced basis methods are snapshot-based, which means that they build\n",
    "the reduced space as a linear subspace of the linear span of solutions\n",
    "of the `fom` for certain {{ parameter_values }}. The easiest\n",
    "approach is to just pick these values randomly, what we will do in the\n",
    "following. First we define a training set of 25 parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3be5678",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = parameter_space.sample_randomly(25)\n",
    "print(training_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f2ca5b",
   "metadata": {},
   "source": [
    "Then we {meth}`~pymor.models.interface.Model.solve` the full-order model\n",
    "for all {{ parameter_values }} in the training set and accumulate all\n",
    "solution vectors in a single {{ VectorArray }} using its\n",
    "{meth}`~pymor.vectorarrays.interface.VectorArray.append` method. But first\n",
    "we need to create an empty {{ VectorArray }} to which the solutions can\n",
    "be appended. New {{ VectorArrays }} in pyMOR are always created by a\n",
    "corresponding {{ VectorSpace }}. Empty arrays are created using the\n",
    "{meth}`~pymor.vectorarrays.interface.VectorSpace.empty` method. But what\n",
    "is the right {{ VectorSpace }}? All {meth}`solutions <pymor.models.interface.Model.solve>`\n",
    "of a {{ Model }} belong to its {attr}`~pymor.models.interface.Model.solution_space`,\n",
    "so we write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c328d834",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = fom.solution_space.empty()\n",
    "for mu in training_set:\n",
    "    training_data.append(fom.solve(mu))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901dc7a8",
   "metadata": {},
   "source": [
    "Note that `fom.solve` returns a {{ VectorArray }} containing a single vector.\n",
    "This entire array (of one vector) is then appended to the `training_data` array.\n",
    "pyMOR has no notion of single vectors, we only speak of {{ VectorArrays }}.\n",
    "\n",
    "What exactly is a {{ VectorSpace }}? A {{ VectorSpace }} in pyMOR holds all\n",
    "information necessary to build {{ VectorArrays }} containing vector objects\n",
    "of a certain type. In our case we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2227a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "fom.solution_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8468c36f",
   "metadata": {},
   "source": [
    "which means that the created {{ VectorArrays }} will internally hold\n",
    "{{ NumPy_arrays }} for data storage. The number is the dimension of the\n",
    "vector. We have here a {{ NumpyVectorSpace }} because pyMOR's built-in\n",
    "discretizations are built around the NumPy/SciPy stack. If `fom` would\n",
    "represent a {{ Model }} living in an external PDE solver, we would have\n",
    "a different type of {{ VectorSpace }} which, for instance, might hold a\n",
    "reference to a discrete functions space object inside the PDE solver\n",
    "instead of the dimension.\n",
    "\n",
    "After appending all solutions vectors to `training_data`, we can verify that\n",
    "`training_data` now really contains 25 vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba6c580",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44331dfe",
   "metadata": {},
   "source": [
    "Note that appending one {{ VectorArray }} `V` to another array `U`\n",
    "will append copies of the vectors in `V` to `U`. So\n",
    "modifying `U` will not affect `V`.\n",
    "\n",
    "Let's look at the solution snapshots we have just computed using\n",
    "the {meth}`~pymor.models.interface.Model.visualize` method of `fom`.\n",
    "A {{ VectorArray }} containing multiple vectors is visualized as a\n",
    "time series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3aeca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fom.visualize(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56876032",
   "metadata": {},
   "source": [
    "## A trivial reduced basis\n",
    "\n",
    "Given some snapshot data, the easiest option to get a reduced basis\n",
    "is to just use the snapshot vectors as the basis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2aac08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trivial_basis = training_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996ce281",
   "metadata": {},
   "source": [
    "Note that assignment in Python never copies data! Thus, if we had written\n",
    "`trivial_basis = training_data` and modified `trivial_basis`, `training_data` would\n",
    "change as well, since `trivial_basis` and `training_data` would refer to the same\n",
    "{{ VectorArray }} object. So whenever you want to use one {{ VectorArray }}\n",
    "somewhere else and you are unsure whether some code might change the\n",
    "array, you should always create a copy. pyMOR uses copy-on-write semantics\n",
    "for copies of {{ VectorArrays }}, which means that generally calling\n",
    "{meth}`~pymor.vectorarrays.interface.VectorArray.copy` is cheap as it does\n",
    "not duplicate any data in memory. Only when you modify one of the arrays\n",
    "the data will be copied.\n",
    "\n",
    "Now we want to know how good our reduced basis is. So we want to compute\n",
    "\n",
    "$$ \\inf_{v \\in V_N}\\ \\|u(\\mu) - v\\|, $$\n",
    "\n",
    "where {math}`V_N` denotes the span of our reduced basis, for all\n",
    "{math}`\\mu` in some validation set of {{ parameter_values }}. Assuming that\n",
    "we are in a Hilbert space, we can compute the infimum via orthogonal\n",
    "projection onto {math}`V_N`: in that case, the projection will be the\n",
    "best approximation in {math}`V_N` and the norm of the difference between\n",
    "{math}`u(\\mu)` and its orthogonal projection will be the best-approximation\n",
    "error.\n",
    "\n",
    "So let {math}`v_{proj}` be the orthogonal projection of {math}`v` onto the\n",
    "linear space spanned by the basis vectors {math}`u_i,\\ i=1, \\ldots, N`.\n",
    "By definition this means that {math}`v - v_{proj}` is orthogonal to\n",
    "all {math}`u_i`:\n",
    "\n",
    "$$ (v - v_{proj}, u_i) = 0, \\qquad i = 1, \\ldots, N. $$\n",
    "\n",
    "Let {math}`\\lambda_j`, {math}`j = 1, \\ldots, N` be the coefficients of\n",
    "{math}`v_{proj}` with respect to  this basis, i.e.\n",
    "{math}`\\sum_{j=1}^N \\lambda_j u_j = v_{proj}`. Then:\n",
    "\n",
    "$$ \\sum_{j=1}^N \\lambda_j (u_j, u_i) = (v, u_i), \\qquad i = 1, \\ldots, N.$$\n",
    "\n",
    "With {math}`G_{i,j} := (u_i, u_j)`,\n",
    "{math}`R := [(v, u_1), \\ldots, (v, u_N)]^T` and\n",
    "{math}`\\Lambda := [\\lambda_1, \\ldots, \\lambda_N]^T`, we obtain the\n",
    "linear equation system\n",
    "\n",
    "$$  G \\cdot \\Lambda = R,$$\n",
    "\n",
    "which determines {math}`\\lambda_i` and, thus, {math}`v_{proj}`.\n",
    "\n",
    "Let's assemble and solve this equation system using pyMOR to determine\n",
    "the best-approximation error in `trivial_basis` for some test vector\n",
    "`U` which we take as another random solution of our {{ Model }}:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0ca90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "U = fom.solve(parameter_space.sample_randomly())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b181d224",
   "metadata": {},
   "source": [
    "The matrix {math}`G` of all inner products between vectors in `trivial_basis`\n",
    "is a so called [Gramian matrix](<https://en.wikipedia.org/wiki/Gramian_matrix>).\n",
    "Consequently, every {{ VectorArray }} has a {meth}`~pymor.vectorarrays.interface.VectorArray.gramian`\n",
    "method, which computes precisely this matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4472f31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = trivial_basis.gramian()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7816042",
   "metadata": {},
   "source": [
    "The Gramian is computed w.r.t. the Euclidean inner product. For the\n",
    "right-hand side {math}`R`, we need to compute all (Euclidean) inner\n",
    "products between the vectors in `trivial_basis` and (the single vector in)\n",
    "`U`. For that, we can use the {meth}`~pymor.vectorarrays.interface.VectorArray.inner`\n",
    "method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68cd04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "R = trivial_basis.inner(U)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c5e747",
   "metadata": {},
   "source": [
    "which will give us a {math}`25\\times 1` {{ NumPy_array }} of all inner products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a18960",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "assert R.shape == (25,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c197cb40",
   "metadata": {},
   "source": [
    "Now, we can use {{ NumPy }} to solve the linear equation system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c933c94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = np.linalg.solve(G, R)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915d1cf9",
   "metadata": {},
   "source": [
    "Finally, we need to form the linear combination\n",
    "\n",
    "$$ \\sum_{j=1}^N \\lambda_j u_j = v_{proj} $$\n",
    "\n",
    "using the {meth}`~pymor.vectorarrays.interface.VectorArray.lincomb` method\n",
    "of `trivial_basis`. It expects row vectors of linear coefficients, but\n",
    "`solve` returns column vectors, so we need to take the transpose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c3fbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "U_proj = trivial_basis.lincomb(lambdas.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38eef378",
   "metadata": {},
   "source": [
    "Let's look at `U`, `U_proj` and the difference of both. {{ VectorArrays }} of\n",
    "the same length can simply be subtracted, yielding a new array of the\n",
    "differences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b177d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for some reason U_proj does not carry over from the previous cell\n",
    "U_proj = trivial_basis.lincomb(lambdas.T)\n",
    "fom.visualize((U, U_proj, U - U_proj),\n",
    "              legend=('U', 'U_proj', 'best-approximation err'),\n",
    "              separate_colorbars=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e66302",
   "metadata": {},
   "source": [
    "As you can see, we already have a quite good approximation of `U` with\n",
    "only 25 basis vectors.\n",
    "\n",
    "Now, the Euclidean norm will just work fine in many cases.\n",
    "However, when the full-order model comes from a PDE, it will be usually not the norm\n",
    "we are interested in, and you may get poor results for problems with\n",
    "strongly anisotropic meshes.\n",
    "\n",
    "For our diffusion problem with homogeneous Dirichlet boundaries,\n",
    "the Sobolev semi-norm (of order one) is a natural choice. Among other useful products,\n",
    "{meth}`~pymor.discretizers.builtin.cg.discretize_stationary_cg` already\n",
    "assembled a corresponding inner product {{ Operator }} for us, which is available\n",
    "as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7b6dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fom.h1_0_semi_product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f568bb5",
   "metadata": {},
   "source": [
    "```{note}\n",
    "The `0` in `h1_0_semi_product` refers to the fact that rows and columns of\n",
    "Dirichlet boundary DOFs have been cleared in the matrix of the Operator to\n",
    "make it invertible. This is important for being able to compute Riesz\n",
    "representatives w.r.t. this inner product (required for a posteriori\n",
    "estimation of the ROM error). If you want to compute the {math}`H^1` semi-norm of a\n",
    "function that does not vanish at the Dirichlet boundary, use\n",
    "`fom.h1_semi_product`.\n",
    "```\n",
    "\n",
    "To use `fom.h1_0_semi_product` as an inner product {{ Operator }} for computing the\n",
    "projection error, we can simply pass it as the optional `product` argument to\n",
    "{meth}`~pymor.vectorarrays.interface.VectorArray.gramian` and\n",
    "{meth}`~pymor.vectorarrays.interface.VectorArray.inner`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abed7d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = trivial_basis[:10].gramian(product=fom.h1_0_semi_product)\n",
    "R = trivial_basis[:10].inner(U, product=fom.h1_0_semi_product)\n",
    "lambdas = np.linalg.solve(G, R)\n",
    "U_h1_proj = trivial_basis[:10].lincomb(lambdas.T)\n",
    "\n",
    "fom.visualize((U, U_h1_proj, U - U_h1_proj), separate_colorbars=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb008b13",
   "metadata": {},
   "source": [
    "As you might have guessed, we have additionally opted here to only use the\n",
    "span of the first 10 basis vectors of `trivial_basis`. Like {{ NumPy_arrays }},\n",
    "{{ VectorArrays }} can be sliced and indexed. The result is *always* a\n",
    "{attr}`view <pymor.vectorarrays.interface.VectorArray.is_view>` onto the\n",
    "original data. If the view object is modified, the original array is modified\n",
    "as well.\n",
    "\n",
    "Next we will assess the approximation error a bit more thoroughly, by\n",
    "checking how well the training data is approximated\n",
    "for varying basis sizes.\n",
    "\n",
    "To optimize the computation of the projection matrix and the right-hand\n",
    "side for varying basis sizes, we first compute these for the full basis\n",
    "and then extract appropriate sub-matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4454fbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_proj_errors(basis, V, product):\n",
    "    G = basis.gramian(product=product)\n",
    "    R = basis.inner(V, product=product)\n",
    "    errors = []\n",
    "    for N in range(len(basis) + 1):\n",
    "        if N > 0:\n",
    "            v = np.linalg.solve(G[:N, :N], R[:N, :])\n",
    "        else:\n",
    "            v = np.zeros((0, len(V)))\n",
    "        V_proj = basis[:N].lincomb(v.T)\n",
    "        errors.append(np.max((V - V_proj).norm(product=product)))\n",
    "    return errors\n",
    "\n",
    "trivial_errors = compute_proj_errors(trivial_basis, training_data, fom.h1_0_semi_product)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254c46dd",
   "metadata": {},
   "source": [
    "Here we have used the fact that we can form multiple linear combinations at once by passing\n",
    "multiple rows of linear coefficients to\n",
    "{meth}`~pymor.vectorarrays.interface.VectorArray.lincomb`. The\n",
    "{meth}`~pymor.vectorarrays.interface.VectorArray.norm` method returns a\n",
    "{{ NumPy_array }} of the norms of all vectors in the array with respect to\n",
    "the given inner product {{ Operator }}. When no norm is specified, the Euclidean\n",
    "norms of the vectors are computed.\n",
    "\n",
    "Let's plot the projection errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230f56e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.figure()\n",
    "plt.semilogy(trivial_errors)\n",
    "plt.ylim(1e-5, 1e1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d02a4fb",
   "metadata": {},
   "source": [
    "Good! We see an exponential decay of the error with growing basis size.\n",
    "The error drops to zero at the end as the basis contains all vectors it\n",
    "needs to approximate.\n",
    "\n",
    "We can do better, however. If we want to use a smaller basis than we\n",
    "have snapshots available, just picking the first of these obviously\n",
    "won't be optimal.\n",
    "\n",
    "## Strong greedy algorithm\n",
    "\n",
    "The strong greedy algorithm iteratively builds reduced spaces\n",
    "{math}`V_N` with a small worst-case best approximation error on a\n",
    "training set of solution snapshots by adding, in each iteration, the\n",
    "currently worst-approximated snapshot vector to the basis of {math}`V_N`.\n",
    "\n",
    "We can easily implement it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e24cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strong_greedy(U, product, N):\n",
    "    basis = U.space.empty()\n",
    "\n",
    "    for n in range(N):\n",
    "        # compute projection errors\n",
    "        G = basis.gramian(product)\n",
    "        R = basis.inner(U, product=product)\n",
    "        lambdas = np.linalg.solve(G, R)\n",
    "        U_proj = basis.lincomb(lambdas.T)\n",
    "        errors = (U - U_proj).norm(product)\n",
    "\n",
    "        # extend basis\n",
    "        basis.append(U[np.argmax(errors)])\n",
    "\n",
    "    return basis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3a192e",
   "metadata": {},
   "source": [
    "Obviously, this algorithm is not optimized as we keep computing inner\n",
    "products we already know, but it will suffice for our purposes. Let's\n",
    "compute a reduced basis using the strong greedy algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc57ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy_basis = strong_greedy(training_data, fom.h1_0_product, 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95465a11",
   "metadata": {},
   "source": [
    "We compute the approximation errors for the training data as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f736c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy_errors = compute_proj_errors(greedy_basis, training_data, fom.h1_0_semi_product)\n",
    "\n",
    "plt.figure()\n",
    "plt.semilogy(trivial_errors, label='trivial')\n",
    "plt.semilogy(greedy_errors, label='greedy')\n",
    "plt.ylim(1e-5, 1e1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eba2df6",
   "metadata": {},
   "source": [
    "Indeed, the strong greedy algorithm constructs better bases than the\n",
    "trivial basis construction algorithm. For compact training sets\n",
    "contained in a Hilbert space, it can actually be shown that the greedy\n",
    "algorithm constructs quasi-optimal spaces in the sense that polynomial\n",
    "or exponential decay of the N-widths {math}`d_N` yields similar rates\n",
    "for the worst-case best-approximation errors of the constructed {math}`V_N`.\n",
    "\n",
    "## Orthonormalization required\n",
    "\n",
    "There is one technical problem with both algorithms however: the\n",
    "condition numbers of the Gramians used to compute the projection\n",
    "onto {math}`V_N` explode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19276e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_trivial = trivial_basis.gramian(fom.h1_0_semi_product)\n",
    "G_greedy = greedy_basis.gramian(fom.h1_0_semi_product)\n",
    "trivial_conds, greedy_conds = [], []\n",
    "for N in range(1, len(training_data)):\n",
    "    trivial_conds.append(np.linalg.cond(G_trivial[:N, :N]))\n",
    "    greedy_conds.append(np.linalg.cond(G_greedy[:N, :N]))\n",
    "plt.figure()\n",
    "plt.semilogy(range(1, len(training_data)), trivial_conds, label='trivial')\n",
    "plt.semilogy(range(1, len(training_data)), greedy_conds, label='greedy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab23e35",
   "metadata": {},
   "source": [
    "This is quite obvious as the snapshot data becomes more and\n",
    "more linear dependent the larger it grows.\n",
    "\n",
    "If we would use the bases we just constructed to build a reduced-order model\n",
    "from them, we will quickly get bitten by the limited accuracy of floating-point numbers.\n",
    "\n",
    "There is a simple remedy however: we orthonormalize our bases. The standard\n",
    "algorithm in pyMOR to do so, is a modified\n",
    "{meth}`~pymor.algorithms.gram_schmidt.gram_schmidt` procedure with\n",
    "re-orthogonalization to improve numerical accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dac59aa",
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "gram_schmidt(greedy_basis, product=fom.h1_0_semi_product, copy=False)\n",
    "gram_schmidt(trivial_basis, product=fom.h1_0_semi_product, copy=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c96dcd",
   "metadata": {},
   "source": [
    "The `copy=False` argument tells the algorithm to orthonormalize\n",
    "the given {{ VectorArray }} in-place instead of returning a new array with\n",
    "the orthonormalized vectors.\n",
    "\n",
    "Since the vectors in `greedy_basis` and `trivial_basis` are now orthonormal,\n",
    "their Gramians are identity matrices (up to numerics). Thus, their condition\n",
    "numbers should be near 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cebc3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_trivial = trivial_basis.gramian(fom.h1_0_semi_product)\n",
    "G_greedy = greedy_basis.gramian(fom.h1_0_semi_product)\n",
    "\n",
    "print(f'trivial: {np.linalg.cond(G_trivial)}, '\n",
    "      f'greedy: {np.linalg.cond(G_greedy)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab04be4c",
   "metadata": {},
   "source": [
    "Orthonormalizing the bases does not change their linear span, so\n",
    "best-approximation errors stay the same. Also, we can\n",
    "compute these errors now more easily by exploiting orthogonality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0247b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_proj_errors_orth_basis(basis, V, product):\n",
    "    errors = []\n",
    "    for N in range(len(basis) + 1):\n",
    "        v = V.inner(basis[:N], product=product)\n",
    "        V_proj = basis[:N].lincomb(v)\n",
    "        errors.append(np.max((V - V_proj).norm(product)))\n",
    "    return errors\n",
    "\n",
    "trivial_errors = compute_proj_errors_orth_basis(trivial_basis, training_data, fom.h1_0_semi_product)\n",
    "greedy_errors  = compute_proj_errors_orth_basis(greedy_basis, training_data, fom.h1_0_semi_product)\n",
    "\n",
    "plt.figure()\n",
    "plt.semilogy(trivial_errors, label='trivial')\n",
    "plt.semilogy(greedy_errors, label='greedy')\n",
    "plt.ylim(1e-5, 1e1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af224ef",
   "metadata": {},
   "source": [
    "## Proper Orthogonal Decomposition\n",
    "\n",
    "Another popular method to create a reduced basis out of snapshot data is the so-called\n",
    "Proper Orthogonal Decomposition (POD) which can be seen as a non-centered version of\n",
    "Principal Component Analysis (PCA). First we build a snapshot matrix\n",
    "\n",
    "```{math}\n",
    "A :=\n",
    "\\begin{bmatrix}\n",
    "\\vdots & \\vdots & \\cdots & \\vdots \\\\\n",
    "u(\\mu_1) & u(\\mu_2) & \\cdots & u(\\mu_K)\\\\\n",
    "\\vdots & \\vdots & \\cdots & \\vdots\n",
    "\\end{bmatrix},\n",
    "```\n",
    "\n",
    "where {math}`K` denotes the total number of solution snapshots. Then we compute the SVD\n",
    "of {math}`A`\n",
    "\n",
    "```{math}\n",
    "A = U \\Sigma V^T,\n",
    "```\n",
    "\n",
    "where {math}`\\Sigma` is an {math}`r \\times r`-diagonal matrix, {math}`U` is an {math}`n \\times r`-matrix\n",
    "and {math}`V` is an {math}`K \\times r`-matrix. Here {math}`n` denotes the dimension of the\n",
    "{attr}`~pymor.models.interface.Model.solution_space` and {math}`r` is the rank of {math}`A`.\n",
    "The diagonal entries {math}`\\sigma_i` of {math}`\\Sigma` are the singular values of {math}`A`, which are\n",
    "assumed to be monotonically decreasing. The pairwise orthogonal and normalized\n",
    "columns of {math}`U` and {math}`V` are the left- resp. right-singular vectors of {math}`A`.\n",
    "The {math}`i`-th POD mode is than simply the {math}`i`-th left-singular vector of {math}`A`,\n",
    "i.e. the {math}`i`-th column of {math}`U`. The larger the corresponding singular value is,\n",
    "the more important is this vector for the approximation of the snapshot data. In fact, if we\n",
    "let {math}`V_N` be the span of the first {math}`N` left-singular vectors of {math}`A`, then\n",
    "the following error identity holds:\n",
    "\n",
    "```{math}\n",
    "\\sum_{k = 1}^K \\inf_{v \\in V_N} \\|u(\\mu_k) - v\\|^2 =\\,\n",
    "\\min_{\\substack{W_N \\subseteq V\\\\ \\operatorname{dim}(W_N) \\leq N}} \\sum_{k = 1}^K \\inf_{v \\in W_N} \\|u(\\mu_k) - v\\|^2 =\n",
    "\\sum_{i = N+1}^{r} \\sigma^2\n",
    "```\n",
    "\n",
    "Thus, the linear spaces produced by the POD are actually optimal, albeit in a different\n",
    "error measure: instead of looking at the worst-case best-approximation error over all\n",
    "{{ parameter_values }}, we minimize the {math}`\\ell^2`-sum of all best-approximation errors.\n",
    "So in the mean squared average, the POD spaces are optimal, but there might be {{ parameter_values }}\n",
    "for which the best-approximation error is quite large.\n",
    "\n",
    "So far we completely neglected that the snapshot vectors may lie in a Hilbert space\n",
    "with some given inner product. To account for that, instead of the snapshot matrix\n",
    "{math}`A`, we consider the linear mapping that sends the {math}`i`-th canonical basis vector\n",
    "{math}`e_k` of {math}`\\mathbb{R}^K` to the vector {math}`u(\\mu_k)` in the\n",
    "{attr}`~pymor.models.interface.Model.solution_space`:\n",
    "\n",
    "$$ \\Phi: \\mathbb{R}^K \\to V, \\ e_k \\mapsto u(\\mu_k).$$\n",
    "\n",
    "Also for this finite-rank (hence compact) operator there exists a SVD of the form\n",
    "\n",
    "$$ \\Phi(v) = \\sum_{i=1}^r u_i \\cdot \\sigma_i \\cdot (v_i, v) \\qquad \\forall v \\in \\mathbb{R}^K,$$\n",
    "\n",
    "with orthonormal vectors {math}`u_i` and {math}`v_i` that generalizes the SVD of a matrix.\n",
    "\n",
    "The POD in this more general form is implemented in pyMOR by the\n",
    "{meth}`~pymor.algorithms.pod.pod` method, which can be called as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98036cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pod_basis, pod_singular_values = pod(training_data, product=fom.h1_0_semi_product, modes=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9423ebd",
   "metadata": {},
   "source": [
    "We said that the POD modes (left-singular vectors) are orthonormal with respect to the\n",
    "inner product on the target Hilbert-space. Let's check that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f76aae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.cond(pod_basis.gramian(fom.h1_0_semi_product))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c34f9ab",
   "metadata": {},
   "source": [
    "Now, let us compare how the POD performs against the greedy algorithm in the worst-case\n",
    "best-approximation error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb66cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pod_errors = compute_proj_errors_orth_basis(pod_basis, training_data, fom.h1_0_semi_product)\n",
    "\n",
    "plt.figure()\n",
    "plt.semilogy(trivial_errors, label='trivial')\n",
    "plt.semilogy(greedy_errors, label='greedy')\n",
    "plt.semilogy(pod_errors, label='POD')\n",
    "plt.ylim(1e-5, 1e1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f88761",
   "metadata": {},
   "source": [
    "As it turns out, the POD spaces perform even slightly better than the greedy spaces.\n",
    "Why is that the case? Note that for finite training or validation sets, both considered error measures\n",
    "are equivalent. In particular:\n",
    "\n",
    "```{math}\n",
    "\\sup_{k = 1,\\ldots,K} \\inf_{v \\in V_N} \\|u(\\mu_k) - v\\| \\leq\n",
    "\\left[\\sum_{k = 1}^K \\inf_{v \\in V_N} \\|u(\\mu_k) - v\\|^2\\right]^{1/2} \\leq\n",
    "\\sqrt{K} \\cdot \\sup_{k = 1,\\ldots,K} \\inf_{v \\in V_N} \\|u(\\mu_k) - v\\|.\n",
    "```\n",
    "\n",
    "Since POD spaces are optimal in the {math}`\\ell^2`-error, they miss the error of the\n",
    "optimal space in the Kolmogorov sense by at most a factor of {math}`\\sqrt{K}`, which in\n",
    "our case is {math}`5`. On the other hand, the greedy algorithm also only produces\n",
    "quasi-optimal spaces. -- For very large training sets with more complex parameter dependence,\n",
    "differences between both spaces may be more significant.\n",
    "\n",
    "Finally, it is often insightful to look at the POD modes themselves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2989adeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fom.visualize(pod_basis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2a7ac8",
   "metadata": {},
   "source": [
    "As you can see, the first (more important) basis vectors account for the approximation of\n",
    "the solutions in the bulk of the subdomains, whereas the higher modes are responsible for\n",
    "approximating the solutions at the subdomain interfaces.\n",
    "\n",
    "## Weak greedy algorithm\n",
    "\n",
    "Both POD and the strong greedy algorithm require the computation of all\n",
    "{meth}`solutions <pymor.models.interface.Model.solve>` {math}`u(\\mu)`\n",
    "for all {{ parameter_values }} {math}`\\mu` in the `training_set`. So it is\n",
    "clear right from the start that we cannot afford very large training sets.\n",
    "Otherwise we would not be interested in model order reduction\n",
    "in the first place. This is a problem when the number of |parameters|\n",
    "increases and/or the solution depends less uniformly on the |parameters|.\n",
    "\n",
    "Reduced basis methods have a very elegant solution to this problem, which\n",
    "allows training sets that are orders of magnitude larger than the training\n",
    "sets affordable for POD: instead of computing the best-approximation error\n",
    "we only compute a surrogate\n",
    "\n",
    "$$ \\inf_{v \\in V_N} \\|u(\\mu) - v\\| \\approx \\mathcal{E}(\\mu)$$\n",
    "\n",
    "for it. Replacing the best-approximation error by this surrogate in the\n",
    "[strong greedy](#strong-greedy-algorithm) algorithm, we arrive at the\n",
    "{meth}`weak greedy <pymor.algorithms.greedy.weak_greedy>`\n",
    "algorithm. If the surrogate {math}`\\mathcal{E}(\\mu)` is an upper and lower bound\n",
    "to the best-approximation error up to some fixed factor, it can still be shown that the\n",
    "produced reduced spaces are quasi-optimal in the same sense as for the strong greedy\n",
    "algorithm, although the involved constants might be worse, depending on the\n",
    "efficiency of {math}`\\mathcal{E}(\\mu)`.\n",
    "\n",
    "Now here comes the trick: to get a surrogate that can be quickly computed, we can\n",
    "use our current reduced-order model for it. More precisely, we choose {math}`\\mathcal{E}(\\mu)`\n",
    "to be of the form\n",
    "\n",
    "$$ \\mathcal{E}(\\mu):= \\operatorname{Err-Est}(\\operatorname{ROM-Solve}(\\mu), \\mu).$$\n",
    "\n",
    "So to compute the surrogate for fixed {{ parameter_values }} {math}`\\mu`, we first\n",
    "{meth}`~pymor.models.interface.Model.solve` the reduced-order model for the current\n",
    "reduced basis for these {{ parameter_values }} and then compute an estimate for the\n",
    "model order reduction error.\n",
    "\n",
    "We won't go into any further details in this tutorial, but for nice problem classes\n",
    "(linear coercive problems with an affine dependence of the system matrix on the {{ Parameters }}),\n",
    "one can derive a posteriori error estimators for which the equivalence with the best-approximation\n",
    "error can be shown and which can be computed efficiently, independently from the size\n",
    "of the full-order model. Here we will only give a simple example how to use the\n",
    "{meth}`weak greedy <pymor.algorithms.greedy.weak_greedy>` algorithm for our problem at hand.\n",
    "\n",
    "In order to do so, we need to be able to build a reduced-order\n",
    "model with an appropriate error estimator. For the given (linear coercive) thermal block problem\n",
    "we can use {class}`~pymor.reductors.coercive.CoerciveRBReductor`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbd3329",
   "metadata": {},
   "outputs": [],
   "source": [
    "reductor = CoerciveRBReductor(\n",
    "    fom,\n",
    "    product=fom.h1_0_semi_product,\n",
    "    coercivity_estimator=ExpressionParameterFunctional('min(diffusion)', fom.parameters)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e27503",
   "metadata": {},
   "source": [
    "Here `product` specifies the inner product with respect to which we want to compute the\n",
    "model order reduction error. With `coercivity_estimator` we need to specify\n",
    "a function which estimates the coercivity constant of the system matrix with respect to\n",
    "the given inner product. In our case, this is just the minimum of the diffusivities over\n",
    "all subdomains.\n",
    "\n",
    "Now we can call {meth}`~pymor.algorithms.greedy.rb_greedy`, which constructs for us the\n",
    "surrogate {math}`\\mathcal{E}(\\mu)` from `fom` and the `reductor` we just\n",
    "constructed. It then passes this surrogate to the {meth}`~pymor.algorithms.greedy.weak_greedy`\n",
    "method. Furthermore, we need to specify the number of basis vectors we want to compute\n",
    "(we could also have specified an error tolerance) and the training set.\n",
    "As the surrogate is cheap to evaluate, we choose here a training set of 1000 different\n",
    "{{ parameter_values }}:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87548f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy_data = rb_greedy(fom, reductor, parameter_space.sample_randomly(1000),\n",
    "                        max_extensions=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7b5b14",
   "metadata": {},
   "source": [
    "Take a look at the log output to see how the basis is built iteratively using\n",
    "the surrogate {math}`\\mathcal{E}(\\mu)`.\n",
    "\n",
    "The returned `greedy_data` dictionary contains various information about the run\n",
    "of the algorithm, including the final ROM. Here, however, we are interested in the\n",
    "generated reduced basis, which is managed by the `reductor`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581afaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "weak_greedy_basis = reductor.bases['RB']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5dbc3e",
   "metadata": {},
   "source": [
    "Let's see, how the weak-greedy basis performs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18885048",
   "metadata": {},
   "outputs": [],
   "source": [
    "weak_greedy_errors = compute_proj_errors_orth_basis(weak_greedy_basis, training_data, fom.h1_0_semi_product)\n",
    "\n",
    "plt.figure()\n",
    "plt.semilogy(trivial_errors, label='trivial')\n",
    "plt.semilogy(greedy_errors, label='greedy')\n",
    "plt.semilogy(pod_errors, label='POD')\n",
    "plt.semilogy(weak_greedy_errors, label='weak greedy')\n",
    "plt.ylim(1e-7, 1e2)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b90aedc",
   "metadata": {},
   "source": [
    "We see that the weak-greedy basis is worse than all the other bases!\n",
    "How can that be?\n",
    "Remember that the weak-greedy basis was trained for a new training data set of 1000 parameter values\n",
    "and not the original much smaller training data set.\n",
    "So what we actually see here is that the other bases are very good at approximating this small training\n",
    "data set, but likely do not generalize well to the entire solution manifold\n",
    "{math}`\\mathcal{M}`.\n",
    "\n",
    "To check this, we compute an additional validation data set of 100 new parameter values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be234902",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_set = parameter_space.sample_randomly(100)\n",
    "validation_data = fom.solution_space.empty()\n",
    "for mu in validation_set:\n",
    "    validation_data.append(fom.solve(mu))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf660b40",
   "metadata": {},
   "source": [
    "Let's see how the approximation error decays on the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a2ef55",
   "metadata": {},
   "outputs": [],
   "source": [
    "trivial_errors = compute_proj_errors_orth_basis(trivial_basis, validation_data, fom.h1_0_semi_product)\n",
    "greedy_errors  = compute_proj_errors_orth_basis(greedy_basis, validation_data, fom.h1_0_semi_product)\n",
    "pod_errors = compute_proj_errors_orth_basis(pod_basis, validation_data, fom.h1_0_semi_product)\n",
    "weak_greedy_errors = compute_proj_errors_orth_basis(weak_greedy_basis, validation_data, fom.h1_0_semi_product)\n",
    "\n",
    "plt.figure()\n",
    "plt.semilogy(trivial_errors, label='trivial')\n",
    "plt.semilogy(greedy_errors, label='greedy')\n",
    "plt.semilogy(pod_errors, label='POD')\n",
    "plt.semilogy(weak_greedy_errors, label='weak greedy')\n",
    "plt.ylim(1e-2, 5e1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded07763",
   "metadata": {},
   "source": [
    "Indeed we see that the weak-greedy basis generalizes to the validation data much better than all the other bases.\n",
    "\n",
    "Download the code:\n",
    "{download}`tutorial_basis_generation.md`\n",
    "{nb-download}`tutorial_basis_generation.ipynb`"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.16.2"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
