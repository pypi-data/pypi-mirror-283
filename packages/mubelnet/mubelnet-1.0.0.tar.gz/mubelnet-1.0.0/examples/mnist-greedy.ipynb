{
   "cells": [
      {
         "cell_type": "code",
         "execution_count": 1,
         "id": "1c8623ce",
         "metadata": {},
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "2023-09-07 12:12:17.278214: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
               ]
            }
         ],
         "source": [
            "from functools import partial\n",
            "\n",
            "from matplotlib import pyplot as plt\n",
            "import numpy as np\n",
            "from sklearn import datasets\n",
            "from sklearn.model_selection import train_test_split\n",
            "\n",
            "# Run the following before any XLA modules such as JAX:\n",
            "import chex\n",
            "\n",
            "chex.set_n_cpu_devices(2)\n",
            "\n",
            "import sys\n",
            "sys.path.append(\"../src\")\n",
            "\n",
            "# Import the remaining JAX related \n",
            "from gabenet.mcmc import greedy_wise_training\n",
            "from gabenet.nets import MultinomialDirichletBelieve\n",
            "from gabenet.utils import freeze_trainable_states, holdout_split, perplexity\n",
            "\n",
            "import haiku as hk\n",
            "import jax\n",
            "from jax import random\n",
            "import jax.numpy as jnp"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "id": "0234db6d",
         "metadata": {},
         "source": [
            "To illustrate how to use the multinomial-Dirichlet believe network, we will train the\n",
            "model on the MNIST dataset, containing handwritten digits.\n",
            "\n",
            "The dataset can directly be loaded from scikit-learn. As preprocessing step, we reshape the\n",
            "digits from a 8x8 square matrix to a flat array of size 64."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 2,
         "id": "b51a7b7b",
         "metadata": {},
         "outputs": [],
         "source": [
            "digits = datasets.load_digits()\n",
            "n_samples = len(digits.images)\n",
            "X = digits.images.reshape((n_samples, -1))\n",
            "X_train, X_test = train_test_split(X, test_size=0.2, random_state=0)"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "id": "b2a8f4f6",
         "metadata": {},
         "source": [
            "Next, we define the model. We use a simple decoder network with two hidden layers. In\n",
            "total, the size of the network: 2 x 10 x 64.\n",
            "\n",
            "```\n",
            "n_hidden_units = (2, 10)\n",
            "model = MultinomialDirichletBelieve(n_hidden_units, n_features)\n",
            "```\n",
            "\n",
            "This function has to be defined in a [haiku](https://github.com/deepmind/dm-haiku) context to transform the network in a pure state for JAX.\n",
            "\n",
            "Since the network is a Bayesian model, we don't train the model by\n",
            "minimising a loss. Rather, we infer the distribution $p(\\boldsymbol{\\theta}|\\boldsymbol{X}_{\\mathrm{train}})$ of the model's parameters $\\boldsymbol{\\theta}$ given the training data $\\boldsymbol{X}_{\\mathrm{train}}$ that we observe. This probability distribution is called the [posterior](https://en.wikipedia.org/wiki/Posterior_probability).\n",
            "\n",
            "Unfortunately, we don't know what this distribution is. However, we do know a way how to sample it: using Markov chain Monte Carlo (MCMC). This simulation method samples the distributions by taking small steps that depend on its previous state. In theory, when we have take enough steps, the state converges to the true (posterior) distribution.\n",
            "\n",
            "First, initialise the chain using training data:\n",
            "\n",
            "```python\n",
            "model.init(X_train)\n",
            "```\n",
            "\n",
            "This method takes samples from the prior as a starting point. After that, keep taking steps from your current to your next state. To take one step, you simply call your model using\n",
            "the training data:\n",
            "\n",
            "```python\n",
            "model(X_train)\n",
            "```\n",
            "\n",
            "This function call does one Gibbs sampling step, which updates all the parameters one-by-one.\n",
            "\n",
            "Now, lets put all elements together."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 3,
         "id": "5ab28182",
         "metadata": {},
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
               ]
            }
         ],
         "source": [
            "# Pseudo-random number generator sequence.\n",
            "key_seq = hk.PRNGSequence(42)\n",
            "\n",
            "m_samples, n_features = X_train.shape\n",
            "\n",
            "@hk.transform_with_state\n",
            "def kernel(n_hidden_units: tuple, X=X_train, training=True):\n",
            "    \"\"\"Advance the Markov chain by one step.\"\"\"\n",
            "    model = MultinomialDirichletBelieve(n_hidden_units, n_features)\n",
            "    # Do one Gibbs sampling step.\n",
            "    model(X)"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "id": "0d4cbcc7",
         "metadata": {},
         "source": [
            "Here, we defined a function that proposes a new state based on its current configuration. This is called a _kernel_. The `hk.transform_with_state` decorator uses haiku to purify the function into something that is stateless."
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "id": "56d21ade",
         "metadata": {},
         "source": [
            "Finally, we draw samples from the Markov chain. We first take 100 burn-in steps, in the\n",
            "hope that the chain converges to the true distribution. After throwing away these first 100 samples, we collect a new set\n",
            "of 50 samples (25 in each chain) to estimate the posterior distribution.\n",
            "\n",
            "Note that `sample_markov_chain` (below) automatically takes care of distributing your\n",
            "computation across multiple devices. For simplicity, we assume you are running on a CPU and split the CPU up in two virtual devices. (See above, at the import section, where we've used\n",
            "`chex` set the number of devices to 2.)\n",
            "\n",
            "The following cell, that collects statistics from the Markov chain takes, about `10 minutes` to run on a CPU."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 4,
         "id": "4b2b6ceb",
         "metadata": {},
         "outputs": [],
         "source": [
            "from functools import partial\n",
            "\n",
            "import haiku as hk\n",
            "import jax\n",
            "import jax.numpy as jnp\n",
            "from jax import random\n",
            "from jaxtyping import Array, PRNGKeyArray, PyTree, UInt\n",
            "\n",
            "from gabenet._surgery import (\n",
            "    copy_to_larger_net,\n",
            "    determine_number_to_prune,\n",
            "    prune_network,\n",
            ")\n",
            "from gabenet.utils import get_hidden_unit_sizes\n",
            "from gabenet.mcmc import sample_markov_chain"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 13,
         "id": "23f32c2f",
         "metadata": {},
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "WARNING:root:Number of trials is fixed at n_trials = Traced<ShapedArray(float32[1437])>with<DynamicJaxprTrace(level=0/1)>\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Starting training network config: (10,)\n",
                  "Pruned network to (7,)\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "WARNING:root:Number of trials is fixed at n_trials = Traced<ShapedArray(float32[1437])>with<DynamicJaxprTrace(level=0/1)>\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Starting training network config: (7, 7)\n",
                  "Pruned network to (6, 7)\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "WARNING:root:Number of trials is fixed at n_trials = Traced<ShapedArray(float32[1437])>with<DynamicJaxprTrace(level=0/1)>\n"
               ]
            }
         ],
         "source": [
            "n_chains = jax.device_count()\n",
            "\n",
            "params, states = greedy_wise_training(\n",
            "    next(key_seq),\n",
            "    kernel=kernel,\n",
            "    n_samples=100,\n",
            "    T_max=2,\n",
            "    K_1max=10,\n",
            "    B=(1_000, 500),\n",
            "    C=(500, 500),\n",
            ")"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "id": "cc6c1598",
         "metadata": {},
         "source": [
            "After training the model, we can inspect what the model has learned. Note that, instead of a single point estimate of the parameters, we've obtained a distribution. To visualise the parameters, we take for simplicity the median. Let's take a look at $\\bm{\\Phi}^{(1)}$, the weights of the first layer."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 14,
         "id": "e09b1bda",
         "metadata": {},
         "outputs": [],
         "source": [
            "states_shallow = states[(7,)]\n",
            "states_deep = states[(6, 7)]"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 15,
         "id": "07381c22",
         "metadata": {},
         "outputs": [],
         "source": [
            "def probability(params, state):\n",
            "    bottom_params = params.get(\"multinomial_dirichlet_believe/~/multinomial_layer\", {})\n",
            "    bottom_state = state[\"multinomial_dirichlet_believe/~/multinomial_layer\"]\n",
            "    phi = bottom_params.get(\"phi\", bottom_state.get(\"phi\"))\n",
            "    theta = bottom_state[\"copy[theta(1)]\"]\n",
            "    return theta @ phi"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 18,
         "id": "bd40335a",
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Training set perplexity: 34.80\n"
               ]
            }
         ],
         "source": [
            "# probs_train = jnp.mean(probability({}, states_shallow), axis=(0, 1))\n",
            "probs_train = jnp.mean(probability({}, states_shallow)[1], axis=0)\n",
            "print(f'Training set perplexity: {perplexity(X_train, probs_train):.2f}')"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 17,
         "id": "dad098e5",
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "<Figure size 640x480 with 0 Axes>"
                  ]
               },
               "metadata": {},
               "output_type": "display_data"
            },
            {
               "data": {
                  "image/png": "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",
                  "text/plain": [
                     "<Figure size 1250x600 with 6 Axes>"
                  ]
               },
               "metadata": {},
               "output_type": "display_data"
            },
            {
               "data": {
                  "text/plain": [
                     "<Figure size 640x480 with 0 Axes>"
                  ]
               },
               "metadata": {},
               "output_type": "display_data"
            },
            {
               "data": {
                  "image/png": "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",
                  "text/plain": [
                     "<Figure size 1250x600 with 6 Axes>"
                  ]
               },
               "metadata": {},
               "output_type": "display_data"
            }
         ],
         "source": [
            "# Aggregate posterior samples to visualise.\n",
            "phi_1st_layer = np.median(states_shallow['multinomial_dirichlet_believe/~/multinomial_layer']['phi'], axis=[1])\n",
            "\n",
            "for i in range(n_chains):  # For each Markov chain.\n",
            "    plt.figure()\n",
            "    _, axes = plt.subplots(nrows=2, ncols=3, figsize=(12.5, 6))\n",
            "    plt.suptitle(f'Chain {i+1}')\n",
            "    # Plot the weights of all 10 hidden states.\n",
            "    for ax, phi in zip(axes.flatten(), phi_1st_layer[i]):\n",
            "        ax.set_axis_off()\n",
            "        image = phi.reshape(8, 8)\n",
            "        ax.imshow(image, cmap=plt.cm.gray_r, interpolation=\"nearest\")"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "4f16dad0",
         "metadata": {},
         "source": [
            "# Test perplexity.\n",
            "Given a distribution $p(x)$, perplexity is defined as $\\mathcal{L} = \\exp(S[p])$ with $S[p]$ the entropy of distribution $p(x)$. In practice, we evaluate the cross entropy with the emperical distribution so that\n",
            "$$\n",
            "\\ln \\mathcal{L} = -E_{\\mathrm{\\pmb{x}}}[ \\ln p(\\pmb{x}|\\pmb{X}_{\\mathrm{train}})] \\approx -\\frac{1}{m}\\sum_{i=1}^{m} \\ln p(\\pmb{x}^{(i)}|\\pmb{X}_{\\mathrm{train}})\n",
            "$$"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "388eefe4",
         "metadata": {},
         "source": [
            "To this end, we split the data in three sets: A training set $\\pmb{X}_{\\mathrm{train}}$, and two tests sets $\\pmb{X}_{\\mathrm{test}}^{A}$ and $\\pmb{X}_{\\mathrm{test}}^{B}$ containing 50% of the pixel intensities each (so that $\\pmb{X}_{\\mathrm{test}} = \\pmb{X}_{\\mathrm{test}}^A + \\pmb{X}_{\\mathrm{test}}^B$), similar to Ref. [1]. \n",
            "\n",
            "[1]: Wang, Chong, John Paisley, and David M. Blei. \"Online variational inference for the hierarchical Dirichlet process.\" Proceedings of the fourteenth international conference on artificial intelligence and statistics. JMLR Workshop and Conference Proceedings, 2011."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 7,
         "id": "a3e1b393",
         "metadata": {},
         "outputs": [],
         "source": [
            "X_test_A, X_test_B = holdout_split(next(key_seq), X_test)"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "a1705bc0",
         "metadata": {},
         "source": [
            "We then use $\\pmb{X}_{\\mathrm{train}}$ to infer the sample independent parameters $\\bm{\\phi}, \\bm{r}, c$ (i.e., without sample index $i$). With these, we use $\\pmb{X}_{\\mathrm{test}}^A $ to infer the sample _dependent_ parameters $\\pmb{\\theta}$ (i.e., with index $i$). Finally, we use $\\pmb{X}_{\\mathrm{test}}^{B}$ to estimate $p(\\pmb{x}|\\pmb{X}_{\\mathrm{train}})$."
         ]
      },
      {
         "cell_type": "markdown",
         "id": "40b69a8d",
         "metadata": {},
         "source": [
            "\n",
            "Specifically, we model the probabilities as categories $p(x_{ij}|\\pi_{ij}) = \\pi_{ij}^{x_{ij}}$ where $\\pi_{ij}$ is the probability to observe pixel intensity $j$ in sample $i$."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 8,
         "id": "616ab655",
         "metadata": {},
         "outputs": [],
         "source": [
            "def probability(params, state):\n",
            "    bottom_params = params.get(\"multinomial_dirichlet_believe/~/multinomial_layer\", {})\n",
            "    bottom_state = state[\"multinomial_dirichlet_believe/~/multinomial_layer\"]\n",
            "    phi = bottom_params.get(\"phi\", bottom_state.get(\"phi\"))\n",
            "    theta = bottom_state[\"theta\"]\n",
            "    return theta @ phi"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "df4c9849",
         "metadata": {},
         "source": [
            "We estimate $\\pi_{ij}$ by averaging the posterior samples:\n",
            "$$\n",
            "\\pi_{ij}(\\pmb{X}_{\\mathrm{train}}, \\pmb{X}_{\\mathrm{test}}^A ) \n",
            "= \\int \\mathrm{d}\\pmb{\\phi} \\, \\mathrm{d}\\pmb{\\theta} \\, p(x_{ij}|\\pmb{\\phi}\\pmb{\\theta}) p(\\pmb{\\theta} | \\pmb{\\phi}, \\pmb{r}, c, \\pmb{X}_{\\mathrm{test}}^A ) p(\\pmb{\\phi}, \\pmb{r}, c|\\pmb{X}_{\\mathrm{train}}) \n",
            "\\approx \\frac{1}{S R} \\sum_{s=1}^S \\sum_{r=1}^{R} \\sum_{k=1}^K \\theta_{ik}^{(s, r)} \\phi_{kj}^{(s)}\n",
            "$$\n",
            "where $\\phi_{kj}^{(s)}$ with $s=1,\\dots,S$ denote posterior samples based on the training set $\\pmb{X}_{\\mathrm{train}}$ and $\\theta_{ik}^{(s, r)}$ the posterior samples $r=1,\\dots,R$ with respect to $\\pmb{X}_{\\mathrm{test}}^A $ given $\\phi_{kj}^{(s)}$ .\n",
            "\n",
            "Having inferred $\\pmb{\\phi}|\\pmb{X}_{\\mathrm{train}}$ above, let's infer $\\pmb{\\theta} | \\pmb{\\phi}, \\pmb{r}, c, \\pmb{X}_{\\mathrm{test}}^A$ for all the different realisations."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 10,
         "id": "cb19fe8e",
         "metadata": {},
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "WARNING:root:Number of trials is fixed at n_trials = Traced<ShapedArray(float32[360])>with<DynamicJaxprTrace(level=1/0)>\n"
               ]
            }
         ],
         "source": [
            "# Lift our the parameters r, c, and phi to keep them fixed.\n",
            "test_params, _ = freeze_trainable_states(states)\n",
            "\n",
            "# Configure Markov chain kernel for X_test_a with clamped parameters (training=False).\n",
            "init_test_fn = jax.jit(jax.vmap(partial(kernel.init, X=X_test_A, training=False)))\n",
            "kernel_test_fn = partial(kernel.apply, X=X_test_A, training=False)\n",
            "\n",
            "# Do inference on X_test_A for each posterior sample individually.\n",
            "probs = []\n",
            "for i in range(n_chains):\n",
            "    probs_i = []\n",
            "    for j in range(25):\n",
            "        params_s = jax.tree_util.tree_map(lambda x: x[i, j], test_params)\n",
            "        # Randomly initialise Markov chain.\n",
            "        keys_per_chain = random.split(next(key_seq), num=n_chains)\n",
            "        _, states_test_s_r = init_test_fn(keys_per_chain)\n",
            "        # Sample Markov chain.\n",
            "        _, states_test_s_r = sample_markov_chain(\n",
            "            next(key_seq), \n",
            "            n_samples=10, \n",
            "            kernel=kernel_test_fn, \n",
            "            params=params_s, \n",
            "            initial_state=states_test_s_r,\n",
            "            n_burnin_steps=20, \n",
            "        )\n",
            "        _ = states_test_s_r['multinomial_dirichlet_believe/~/cap_layer']['theta'].block_until_ready()\n",
            "        probs_ij = probability(params_s, states_test_s_r)\n",
            "        probs_i.append(probs_ij)\n",
            "    probs.append(jnp.stack(probs_i))\n",
            "        \n",
            "probs = jnp.stack(probs)"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "cace3019",
         "metadata": {},
         "source": [
            "Finally, the per intensity perplexity is computed on the separate hold-out set $\\pmb{X}_{\\mathrm{test}}^B$:\n",
            "$$\n",
            "\\mathcal{L}\\left(\\pmb{X}_{\\mathrm{test}}^B| \\pmb{\\pi}\\right) = \\exp \\left(-\\frac{1}{m} \\sum_{i=1}^m \\sum_{j=1}^n \\frac{x_{ij} \\ln {\\pi}_{ij}}{\\sum_{j=1}^n x_{ij}} \\right).\n",
            "$$"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 11,
         "id": "5615dd9c",
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Training set perplexity: 33.80\n"
               ]
            }
         ],
         "source": [
            "probs_train = jnp.mean(probability(params, states), axis=(0, 1))\n",
            "print(f'Training set perplexity: {perplexity(X_train, probs_train):.2f}')"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 12,
         "id": "cee51019",
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Test set perplexity: 34.98\n"
               ]
            }
         ],
         "source": [
            "probs_test = probs.mean(axis=[0, 1, 2, 3])\n",
            "print(f'Test set perplexity: {perplexity(X_test_B, probs_test):.2f}')"
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "Python 3 (ipykernel)",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.11.4"
      },
      "vscode": {
         "interpreter": {
            "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
         }
      }
   },
   "nbformat": 4,
   "nbformat_minor": 5
}
