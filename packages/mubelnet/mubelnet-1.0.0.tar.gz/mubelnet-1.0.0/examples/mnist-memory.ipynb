{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c8623ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-11 12:44:43.342618: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "\n",
    "# Run the following before any XLA modules such as JAX:\n",
    "import chex\n",
    "\n",
    "# chex.set_n_cpu_devices(2)\n",
    "\n",
    "# Add end-to-end package to path.\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(str(Path('../src').absolute()))\n",
    "\n",
    "# Import the remaining JAX related \n",
    "from gabenet.mcmc import sample_markov_chain\n",
    "from gabenet.nets import MultinomialDirichletBelieve\n",
    "\n",
    "import haiku as hk"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0234db6d",
   "metadata": {},
   "source": [
    "To illustrate how to use the multinomial-Dirichlet believe network, we will train the\n",
    "model on the MNIST dataset, containing handwritten digits.\n",
    "\n",
    "The dataset can directly be loaded from scikit-learn. As preprocessing step, we reshape the\n",
    "digits from a 8x8 square matrix to a flat array of size 64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b51a7b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = datasets.load_digits()\n",
    "n_samples = len(digits.images)\n",
    "X_train = digits.images.reshape((n_samples, -1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b2a8f4f6",
   "metadata": {},
   "source": [
    "Next, we define the model. We use a simple decoder network with two hidden layers. In\n",
    "total, the size of the network: 2 x 10 x 64.\n",
    "\n",
    "```\n",
    "n_hidden_units = (2, 10)\n",
    "model = MultinomialDirichletBelieve(n_hidden_units, n_features)\n",
    "```\n",
    "\n",
    "This function has to be defined in a [haiku](https://github.com/deepmind/dm-haiku) context to transform the network in a pure state for JAX.\n",
    "\n",
    "Since the network is a Bayesian model, we don't train the model by\n",
    "minimising a loss. Rather, we infer the distribution $p(\\boldsymbol{\\theta}|\\boldsymbol{X}_{\\mathrm{train}})$ of the model's parameters $\\boldsymbol{\\theta}$ given the training data $\\boldsymbol{X}_{\\mathrm{train}}$ that we observe. This probability distribution is called the [posterior](https://en.wikipedia.org/wiki/Posterior_probability).\n",
    "\n",
    "Unfortunately, we don't know what this distribution is. However, we do know a way how to sample it: using Markov chain Monte Carlo (MCMC). This simulation method samples the distributions by taking small steps that depend on its previous state. In theory, when we have take enough steps, the state converges to the true (posterior) distribution.\n",
    "\n",
    "First, initialise the chain using training data:\n",
    "\n",
    "```python\n",
    "model.init(X_train)\n",
    "```\n",
    "\n",
    "This method takes samples from the prior as a starting point. After that, keep taking steps from your current to your next state. To take one step, you simply call your model using\n",
    "the training data:\n",
    "\n",
    "```python\n",
    "model(X_train)\n",
    "```\n",
    "\n",
    "This function call does one Gibbs sampling step, which updates all the parameters one-by-one.\n",
    "\n",
    "Now, lets put all elements together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ab28182",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "# Pseudo-random number generator sequence.\n",
    "key_seq = hk.PRNGSequence(42)\n",
    "\n",
    "m_samples, n_features = X_train.shape\n",
    "\n",
    "@hk.transform_with_state\n",
    "def kernel():\n",
    "    \"\"\"Advance the Markov chain by one step.\"\"\"\n",
    "    n_hidden_units = (10, )\n",
    "    model = MultinomialDirichletBelieve(n_hidden_units, n_features)\n",
    "    if hk.running_init():\n",
    "        # Initialise Markov chain.\n",
    "        model.init(X_train)\n",
    "\n",
    "    # Do one Gibbs sampling step.\n",
    "    model(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71b8a917",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Number of trials is fixed at n_trials = [1797. 1798. 1799. ... 3591. 3592. 3593.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Array([5.1007801e-01, 3.7258615e-25, 6.6692219e-03, 5.5523387e-08,\n",
       "       5.0742415e-08, 1.1700398e-01, 2.1399991e-17, 3.6347800e-01,\n",
       "       1.1877611e-09, 2.7706921e-03], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params, state = kernel.init(next(key_seq))\n",
    "state['multinomial_dirichlet_believe/~/cap_layer']['r'].block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe805260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([5.0151855e-01, 3.2134609e-05, 7.0774397e-03, 3.1438991e-16,\n",
       "       6.3737040e-15, 1.2054221e-01, 1.8244776e-06, 3.6653900e-01,\n",
       "       9.0008516e-06, 4.2798021e-03], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params, state = kernel.apply(params, state, next(key_seq))\n",
    "state['multinomial_dirichlet_believe/~/cap_layer']['r'].block_until_ready()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0d4cbcc7",
   "metadata": {},
   "source": [
    "Here, we defined a function that proposes a new state based on its current configuration. This is called a _kernel_. The `hk.transform_with_state` decorator uses haiku to purify the function into something that is stateless."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "56d21ade",
   "metadata": {},
   "source": [
    "Finally, we draw samples from the Markov chain. We first take 100 burn-in steps, in the\n",
    "hope that the chain converges to the true distribution. After throwing away these first 100 samples, we collect a new set\n",
    "of 100 samples (50 in each chain) to estimate the posterior distribution.\n",
    "\n",
    "Note that `sample_markov_chain` (below) automatically takes care of distributing your\n",
    "computation across multiple devices. For simplicity, we assume you are running on a CPU and split the CPU up in two virtual devices. (See above, at the import section, where we've used\n",
    "`chex` set the number of devices to 2.)\n",
    "\n",
    "The following cell, that collects statistics from the Markov chain takes, about `10 minutes` to run on a CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "682f6263",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Number of trials is fixed at n_trials = [1797. 1798. 1799. ... 3591. 3592. 3593.]\n",
      "/home/hylke/workspace/gabenet/examples/../src/gabenet/mcmc.py:95: UserWarning: Only one visible device in JAX. Reconfigure XLA_FLAGS.\n",
      "  warnings.warn(\"Only one visible device in JAX. Reconfigure XLA_FLAGS.\")\n"
     ]
    }
   ],
   "source": [
    "params, states = sample_markov_chain(\n",
    "    next(key_seq),\n",
    "    kernel=kernel,\n",
    "    n_samples=50, \n",
    "    n_burnin_steps=50, \n",
    "    n_chains=1,\n",
    ")\n",
    "\n",
    "_ = states['multinomial_dirichlet_believe/~/multinomial_layer']['phi'].block_until_ready()\n",
    "# jax.profiler.save_device_memory_profile('/tmp/mcmc/sample.prof')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cc6c1598",
   "metadata": {},
   "source": [
    "After training the model, we can inspect what the model has learned. Note that, instead of a single point estimate of the parameters, we've obtained a distribution. To visualise the parameters, we take for simplicity the median. Let's take a look at $\\bm{\\Phi}^{(1)}$, the weights of the first layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cc1ebdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Number of trials is fixed at n_trials = [1797. 1798. 1799. ... 3591. 3592. 3593.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:jax._src.dispatch:Finished tracing + transforming jit(while) in 0.00032639503479003906 sec\n",
      "WARNING:jax._src.interpreters.pxla:Compiling while for with global shapes and types [ShapedArray(float32[1797,64]), ShapedArray(uint32[64,2]), ShapedArray(int32[]), ShapedArray(float32[1,1797]), ShapedArray(float32[1797]), ShapedArray(float32[64,1,1797])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).\n",
      "WARNING:jax._src.dispatch:Finished jaxpr to MLIR module conversion jit(while) in 0.5246031284332275 sec\n",
      "WARNING:jax._src.dispatch:Finished XLA compilation of jit(while) in 1.8872430324554443 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:jax._src.dispatch:Finished tracing + transforming jit(broadcast_in_dim) in 0.0002994537353515625 sec\n",
      "WARNING:jax._src.interpreters.pxla:Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).\n",
      "WARNING:jax._src.dispatch:Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.001613616943359375 sec\n",
      "WARNING:jax._src.dispatch:Finished XLA compilation of jit(broadcast_in_dim) in 0.012001752853393555 sec\n",
      "WARNING:jax._src.dispatch:Finished tracing + transforming jit(concatenate) in 0.0009098052978515625 sec\n",
      "WARNING:jax._src.interpreters.pxla:Compiling concatenate for with global shapes and types [ShapedArray(float32[1]), ShapedArray(float32[1]), ShapedArray(float32[1]), ShapedArray(float32[1]), ShapedArray(float32[1]), ShapedArray(float32[1]), ShapedArray(float32[1]), ShapedArray(float32[1]), ShapedArray(float32[1]), ShapedArray(float32[1]), ShapedArray(float32[1]), ShapedArray(float32[1]), ShapedArray(float32[1]), ShapedArray(float32[1]), ShapedArray(float32[1]), ShapedArray(float32[1])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).\n",
      "WARNING:jax._src.dispatch:Finished jaxpr to MLIR module conversion jit(concatenate) in 0.0026483535766601562 sec\n",
      "WARNING:jax._src.dispatch:Finished XLA compilation of jit(concatenate) in 0.01611781120300293 sec\n",
      "WARNING:jax._src.dispatch:Finished tracing + transforming jit(concatenate) in 0.0005764961242675781 sec\n",
      "WARNING:jax._src.interpreters.pxla:Compiling concatenate for with global shapes and types [ShapedArray(float32[1]), ShapedArray(float32[1])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated})).\n",
      "WARNING:jax._src.dispatch:Finished jaxpr to MLIR module conversion jit(concatenate) in 0.01079416275024414 sec\n",
      "WARNING:jax._src.dispatch:Finished XLA compilation of jit(concatenate) in 0.024568796157836914 sec\n",
      "WARNING:jax._src.dispatch:Finished tracing + transforming jit(concatenate) in 0.0005898475646972656 sec\n",
      "WARNING:jax._src.interpreters.pxla:Compiling concatenate for with global shapes and types [ShapedArray(float32[16]), ShapedArray(float32[16]), ShapedArray(float32[16]), ShapedArray(float32[2])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).\n",
      "WARNING:jax._src.dispatch:Finished jaxpr to MLIR module conversion jit(concatenate) in 0.009582042694091797 sec\n",
      "WARNING:jax._src.dispatch:Finished XLA compilation of jit(concatenate) in 0.02217721939086914 sec\n",
      "WARNING:jax._src.dispatch:Finished tracing + transforming jit(concatenate) in 0.0011563301086425781 sec\n",
      "WARNING:jax._src.interpreters.pxla:Compiling concatenate for with global shapes and types [ShapedArray(float32[1,10]), ShapedArray(float32[1,10]), ShapedArray(float32[1,10]), ShapedArray(float32[1,10]), ShapedArray(float32[1,10]), ShapedArray(float32[1,10]), ShapedArray(float32[1,10]), ShapedArray(float32[1,10]), ShapedArray(float32[1,10]), ShapedArray(float32[1,10]), ShapedArray(float32[1,10]), ShapedArray(float32[1,10]), ShapedArray(float32[1,10]), ShapedArray(float32[1,10]), ShapedArray(float32[1,10]), ShapedArray(float32[1,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).\n",
      "WARNING:jax._src.dispatch:Finished jaxpr to MLIR module conversion jit(concatenate) in 0.0075817108154296875 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:jax._src.dispatch:Finished XLA compilation of jit(concatenate) in 0.037312984466552734 sec\n",
      "WARNING:jax._src.dispatch:Finished tracing + transforming jit(concatenate) in 0.001245737075805664 sec\n",
      "WARNING:jax._src.interpreters.pxla:Compiling concatenate for with global shapes and types [ShapedArray(float32[1,10]), ShapedArray(float32[1,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated})).\n",
      "WARNING:jax._src.dispatch:Finished jaxpr to MLIR module conversion jit(concatenate) in 0.006678581237792969 sec\n",
      "WARNING:jax._src.dispatch:Finished XLA compilation of jit(concatenate) in 0.02582073211669922 sec\n",
      "WARNING:jax._src.dispatch:Finished tracing + transforming jit(concatenate) in 0.0007462501525878906 sec\n",
      "WARNING:jax._src.interpreters.pxla:Compiling concatenate for with global shapes and types [ShapedArray(float32[16,10]), ShapedArray(float32[16,10]), ShapedArray(float32[16,10]), ShapedArray(float32[2,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).\n",
      "WARNING:jax._src.dispatch:Finished jaxpr to MLIR module conversion jit(concatenate) in 0.01071619987487793 sec\n",
      "WARNING:jax._src.dispatch:Finished XLA compilation of jit(concatenate) in 0.029124021530151367 sec\n",
      "WARNING:jax._src.dispatch:Finished tracing + transforming jit(broadcast_in_dim) in 0.0006625652313232422 sec\n",
      "WARNING:jax._src.interpreters.pxla:Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[1797,10])]. Argument mapping: (GSPMDSharding({replicated}),).\n",
      "WARNING:jax._src.dispatch:Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.004687070846557617 sec\n",
      "WARNING:jax._src.dispatch:Finished XLA compilation of jit(broadcast_in_dim) in 0.023826122283935547 sec\n",
      "WARNING:jax._src.dispatch:Finished tracing + transforming jit(concatenate) in 0.002017498016357422 sec\n",
      "WARNING:jax._src.interpreters.pxla:Compiling concatenate for with global shapes and types [ShapedArray(float32[1,1797,10]), ShapedArray(float32[1,1797,10]), ShapedArray(float32[1,1797,10]), ShapedArray(float32[1,1797,10]), ShapedArray(float32[1,1797,10]), ShapedArray(float32[1,1797,10]), ShapedArray(float32[1,1797,10]), ShapedArray(float32[1,1797,10]), ShapedArray(float32[1,1797,10]), ShapedArray(float32[1,1797,10]), ShapedArray(float32[1,1797,10]), ShapedArray(float32[1,1797,10]), ShapedArray(float32[1,1797,10]), ShapedArray(float32[1,1797,10]), ShapedArray(float32[1,1797,10]), ShapedArray(float32[1,1797,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).\n",
      "WARNING:jax._src.dispatch:Finished jaxpr to MLIR module conversion jit(concatenate) in 0.0040323734283447266 sec\n",
      "WARNING:jax._src.dispatch:Finished XLA compilation of jit(concatenate) in 0.06602883338928223 sec\n",
      "WARNING:jax._src.dispatch:Finished tracing + transforming jit(concatenate) in 0.0006220340728759766 sec\n",
      "WARNING:jax._src.interpreters.pxla:Compiling concatenate for with global shapes and types [ShapedArray(float32[1,1797,10]), ShapedArray(float32[1,1797,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated})).\n",
      "WARNING:jax._src.dispatch:Finished jaxpr to MLIR module conversion jit(concatenate) in 0.006283760070800781 sec\n",
      "WARNING:jax._src.dispatch:Finished XLA compilation of jit(concatenate) in 0.022881031036376953 sec\n",
      "WARNING:jax._src.dispatch:Finished tracing + transforming jit(concatenate) in 0.0007648468017578125 sec\n",
      "WARNING:jax._src.interpreters.pxla:Compiling concatenate for with global shapes and types [ShapedArray(float32[16,1797,10]), ShapedArray(float32[16,1797,10]), ShapedArray(float32[16,1797,10]), ShapedArray(float32[2,1797,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).\n",
      "WARNING:jax._src.dispatch:Finished jaxpr to MLIR module conversion jit(concatenate) in 0.0037038326263427734 sec\n",
      "WARNING:jax._src.dispatch:Finished XLA compilation of jit(concatenate) in 0.06026625633239746 sec\n",
      "WARNING:jax._src.dispatch:Finished tracing + transforming jit(broadcast_in_dim) in 0.00045108795166015625 sec\n",
      "WARNING:jax._src.interpreters.pxla:Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[10,64])]. Argument mapping: (GSPMDSharding({replicated}),).\n",
      "WARNING:jax._src.dispatch:Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.0030868053436279297 sec\n",
      "WARNING:jax._src.dispatch:Finished XLA compilation of jit(broadcast_in_dim) in 0.01771855354309082 sec\n",
      "WARNING:jax._src.dispatch:Finished tracing + transforming jit(concatenate) in 0.0027124881744384766 sec\n",
      "WARNING:jax._src.interpreters.pxla:Compiling concatenate for with global shapes and types [ShapedArray(float32[1,10,64]), ShapedArray(float32[1,10,64]), ShapedArray(float32[1,10,64]), ShapedArray(float32[1,10,64]), ShapedArray(float32[1,10,64]), ShapedArray(float32[1,10,64]), ShapedArray(float32[1,10,64]), ShapedArray(float32[1,10,64]), ShapedArray(float32[1,10,64]), ShapedArray(float32[1,10,64]), ShapedArray(float32[1,10,64]), ShapedArray(float32[1,10,64]), ShapedArray(float32[1,10,64]), ShapedArray(float32[1,10,64]), ShapedArray(float32[1,10,64]), ShapedArray(float32[1,10,64])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).\n",
      "WARNING:jax._src.dispatch:Finished jaxpr to MLIR module conversion jit(concatenate) in 0.0036208629608154297 sec\n",
      "WARNING:jax._src.dispatch:Finished XLA compilation of jit(concatenate) in 0.025615692138671875 sec\n",
      "WARNING:jax._src.dispatch:Finished tracing + transforming jit(concatenate) in 0.0008792877197265625 sec\n",
      "WARNING:jax._src.interpreters.pxla:Compiling concatenate for with global shapes and types [ShapedArray(float32[1,10,64]), ShapedArray(float32[1,10,64])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated})).\n",
      "WARNING:jax._src.dispatch:Finished jaxpr to MLIR module conversion jit(concatenate) in 0.004022359848022461 sec\n",
      "WARNING:jax._src.dispatch:Finished XLA compilation of jit(concatenate) in 0.02708601951599121 sec\n",
      "WARNING:jax._src.dispatch:Finished tracing + transforming jit(concatenate) in 0.0010828971862792969 sec\n",
      "WARNING:jax._src.interpreters.pxla:Compiling concatenate for with global shapes and types [ShapedArray(float32[16,10,64]), ShapedArray(float32[16,10,64]), ShapedArray(float32[16,10,64]), ShapedArray(float32[2,10,64])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).\n",
      "WARNING:jax._src.dispatch:Finished jaxpr to MLIR module conversion jit(concatenate) in 0.009553909301757812 sec\n",
      "WARNING:jax._src.dispatch:Finished XLA compilation of jit(concatenate) in 0.02797222137451172 sec\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "from typing import Any, Callable, Optional\n",
    "import warnings\n",
    "\n",
    "import haiku as hk\n",
    "import jax\n",
    "from jax import random\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from gabenet.sugar import scannable\n",
    "\n",
    "def sample_markov_chain3(    key,\n",
    "    kernel: Callable[[tuple], tuple[Any, dict]] | hk.TransformedWithState,\n",
    "    n_samples: int,\n",
    "    n_burnin_steps: int,\n",
    "    n_chains: Optional[int] = None,\n",
    "    params: Optional[hk.Params] = None,\n",
    "    initial_state: Optional[hk.State] = None,\n",
    "    n_leap_size: int = 1,\n",
    "):\n",
    "    key_seq = hk.PRNGSequence(key)\n",
    "    n_samples_per_chain = n_samples // n_chains\n",
    "\n",
    "    states = []\n",
    "    params = []\n",
    "    for i in range(n_chains):\n",
    "        param, state = kernel.init(next(key_seq))\n",
    "        print('----------------------')\n",
    "        for _ in range(n_burnin_steps):\n",
    "            _, state = kernel.apply(param, state, next(key_seq))\n",
    "        for _ in range(n_samples_per_chain):\n",
    "            _, state = kernel.apply(param, state, next(key_seq))\n",
    "            states.append(state)\n",
    "        print('---------------------')\n",
    "        params.append(param)\n",
    "\n",
    "    params_tree = jax.tree_util.tree_map(lambda *x: jnp.stack(x), *params)\n",
    "    states_tree = jax.tree_util.tree_map(lambda *x: jnp.stack(x), *states)\n",
    "    return params_tree, states_tree\n",
    "\n",
    "with jax.log_compiles(True):\n",
    "    params, states = sample_markov_chain3(\n",
    "        next(key_seq),\n",
    "        kernel=kernel,\n",
    "        n_samples=50, \n",
    "        n_burnin_steps=50, \n",
    "        n_chains=1,\n",
    "    )\n",
    "\n",
    "    _ = states['multinomial_dirichlet_believe/~/multinomial_layer']['phi'].block_until_ready()\n",
    "# jax.profiler.save_device_memory_profile('/tmp/gabenet/sample2.prof')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2f0ae43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Number of trials is fixed at n_trials = [1797. 1798. 1799. ... 3591. 3592. 3593.]\n",
      "WARNING:jax._src.dispatch:Finished tracing + transforming jit(while) in 0.00027561187744140625 sec\n",
      "WARNING:jax._src.interpreters.pxla:Compiling while for with global shapes and types [ShapedArray(float32[1,1797,64]), ShapedArray(uint32[1,64,2]), ShapedArray(int32[]), ShapedArray(float32[1,1,1797]), ShapedArray(float32[1,1797]), ShapedArray(float32[1,64,1,1797])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).\n",
      "WARNING:jax._src.dispatch:Finished jaxpr to MLIR module conversion jit(while) in 0.7062482833862305 sec\n",
      "WARNING:jax._src.dispatch:Finished XLA compilation of jit(while) in 2.200800657272339 sec\n",
      "WARNING:jax._src.dispatch:Finished tracing + transforming jit(scan) in 0.00032520294189453125 sec\n",
      "WARNING:jax._src.interpreters.pxla:Compiling scan for with global shapes and types [ShapedArray(float32[1]), ShapedArray(float32[1,10]), ShapedArray(float32[1,1797,10]), ShapedArray(float32[1,10,64]), ShapedArray(float32[1,1797,10]), ShapedArray(uint32[1,2])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).\n",
      "WARNING:jax._src.dispatch:Finished jaxpr to MLIR module conversion jit(scan) in 2.9359023571014404 sec\n",
      "WARNING:jax._src.dispatch:Finished XLA compilation of jit(scan) in 14.918276309967041 sec\n",
      "WARNING:jax._src.dispatch:Finished tracing + transforming jit(scan) in 0.0003654956817626953 sec\n",
      "WARNING:jax._src.interpreters.pxla:Compiling scan for with global shapes and types [ShapedArray(float32[1]), ShapedArray(float32[1,10]), ShapedArray(float32[1,1797,10]), ShapedArray(float32[1,10,64]), ShapedArray(float32[1,1797,10]), ShapedArray(uint32[1,2])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).\n",
      "WARNING:jax._src.dispatch:Finished jaxpr to MLIR module conversion jit(scan) in 2.6584057807922363 sec\n",
      "WARNING:jax._src.dispatch:Finished XLA compilation of jit(scan) in 13.632458925247192 sec\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def sample_markov_chain2(\n",
    "    key,\n",
    "    kernel: Callable[[tuple], tuple[Any, dict]] | hk.TransformedWithState,\n",
    "    n_samples: int,\n",
    "    n_burnin_steps: int,\n",
    "    n_chains: Optional[int] = None,\n",
    "    params: Optional[hk.Params] = None,\n",
    "    initial_state: Optional[hk.State] = None,\n",
    "    n_leap_size: int = 1,\n",
    "):\n",
    "\n",
    "    key_seq = hk.PRNGSequence(key)\n",
    "\n",
    "    _scannable_kernel_fn = scannable(kernel)\n",
    "\n",
    "    if initial_state is None or params is None:\n",
    "        if not isinstance(kernel, hk.TransformedWithState):\n",
    "            raise ValueError(\n",
    "                \"Kernel must be `TransformedWithState` to be able to initialise\"\n",
    "                \"`params` and `state` when either is None.\"\n",
    "            )\n",
    "        if n_chains is None:\n",
    "            raise ValueError(\"Number of chains not specified!\")\n",
    "\n",
    "        init_key_per_chain = random.split(next(key_seq), num=n_chains)\n",
    "        params, initial_state = jax.vmap(kernel.init)(init_key_per_chain)\n",
    "\n",
    "\n",
    "    def _leapfrog(carry, _):\n",
    "        \"\"\"A for-loop (from 0,..,`n_steps-1`) that runs `step`.\"\"\"\n",
    "        carry_out, _ = jax.lax.scan(\n",
    "            _scannable_kernel_fn, carry, xs=None, length=n_leap_size\n",
    "        )\n",
    "        state = carry_out[1]\n",
    "        return carry_out, state\n",
    "\n",
    "    def _sampler(params_init, state_init, key, n_sample_size: int):\n",
    "        \"\"\"Samper for a single Markov chain.\"\"\"\n",
    "        carry = (params_init, state_init, key)\n",
    "        # 1) Take first sample after `(n_burnin - n_leap_size) + n_leap_size` steps.\n",
    "        n_steps = n_burnin_steps - n_leap_size\n",
    "        carry, _ = jax.lax.scan(_scannable_kernel_fn, carry, xs=None, length=n_steps)\n",
    "        # 2) Take subsequent samples after `n_leap_size` steps.\n",
    "        _, stacked_states = jax.lax.scan(\n",
    "            _leapfrog, carry, xs=None, length=n_sample_size\n",
    "        )\n",
    "\n",
    "        return stacked_states\n",
    "\n",
    "    n_devices = jax.local_device_count()\n",
    "\n",
    "\n",
    "    # Generate an initial state for each chain.\n",
    "    keys = random.split(next(key_seq), num=n_chains)\n",
    "\n",
    "    n_samples_per_chain = n_samples // n_chains\n",
    "    _vectorised_kernel_fn = jax.vmap(\n",
    "        partial(_sampler, n_sample_size=n_samples_per_chain), in_axes=(0, 0, 0)\n",
    "    )\n",
    "    state = _vectorised_kernel_fn(params, initial_state, keys)\n",
    "    return params, state\n",
    "\n",
    "with jax.log_compiles(True):\n",
    "    params, states = sample_markov_chain2(\n",
    "        next(key_seq),\n",
    "        kernel=kernel,\n",
    "        n_samples=50, \n",
    "        n_burnin_steps=50, \n",
    "        n_chains=1,\n",
    "    )\n",
    "\n",
    "    _ = states['multinomial_dirichlet_believe/~/multinomial_layer']['phi'].block_until_ready()\n",
    "# jax.profiler.save_device_memory_profile('/tmp/gabenet/sample2.prof')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dad098e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1250x600 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1250x600 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Aggregate posterior samples to visualise.\n",
    "phi_1st_layer = np.median(states['multinomial_dirichlet_believe/~/multinomial_layer']['phi'], axis=[1])\n",
    "\n",
    "for i in range(2):  # For each Markov chain.\n",
    "    plt.figure()\n",
    "    _, axes = plt.subplots(nrows=2, ncols=5, figsize=(12.5, 6))\n",
    "    plt.suptitle(f'Chain {i+1}')\n",
    "    # Plot the weights of all 10 hidden states.\n",
    "    for ax, phi in zip(axes.flatten(), phi_1st_layer[i]):\n",
    "        ax.set_axis_off()\n",
    "        image = phi.reshape(8, 8)\n",
    "        ax.imshow(image, cmap=plt.cm.gray_r, interpolation=\"nearest\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
