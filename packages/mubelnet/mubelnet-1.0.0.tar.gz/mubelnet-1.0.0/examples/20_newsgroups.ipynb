{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d199f7ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-17 19:35:59.745256: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "# Import the remaining JAX related \n",
    "from gabenet.mcmc import sample_markov_chain\n",
    "from gabenet.nets import MultinomialDirichletBelieve\n",
    "import haiku as hk\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6a95ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "i_checkpoint = 0\n",
    "\n",
    "ARTEFACT_DIR = Path(os.environ.get('ARTEFACT_DIR', './checkpoints/'))\n",
    "ARTEFACT_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a39b98c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_train = fetch_20newsgroups(subset='all', )\n",
    "cv = CountVectorizer(min_df=10, max_features=2_000)\n",
    "X_train = cv.fit_transform(files_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "317ea883",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[:10].todense().astype(jnp.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ef4bc9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "# Pseudo-random number generator sequence.\n",
    "key_seq = hk.PRNGSequence(42)\n",
    "\n",
    "m_samples, n_features = X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "049b465f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_last_state(states: dict) -> dict:\n",
    "    global i_checkpoint\n",
    "    \"\"\"Extract and dump last state to disk.\"\"\"\n",
    "    last_state = jax.tree_util.tree_map(lambda x: x[:, -1, ...], states)\n",
    "\n",
    "    with open(ARTEFACT_DIR / f'state_{i_checkpoint}.pkl', 'wb') as fo:\n",
    "        pickle.dump(last_state, fo)\n",
    "        print(f'Saving checkpoint i={i_checkpoint}.')\n",
    "\n",
    "    i_checkpoint += 1\n",
    "\n",
    "    del states\n",
    "    \n",
    "    return last_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8078cd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def network_size(m_samples, n_features):\n",
    "    \"\"\"Estimate size of 2-layer mult-dir net.\"\"\"\n",
    "    n_hidden_units = [int(n_features**0.25), int(n_features**0.5)]\n",
    "    \n",
    "    n_theta = (m_samples * n_hidden_units[-1], m_samples * n_hidden_units[-2])\n",
    "    n_phi = [n_features * n_hidden_units[-1], n_hidden_units[-1] * n_hidden_units[-2]]\n",
    "    n_r = n_hidden_units[:1]\n",
    "    n_c = [1]\n",
    "\n",
    "    n_rate = m_samples * np.array([ n_hidden_units[0] * n_hidden_units[1], n_features * n_hidden_units[1]])\n",
    "    n_m = m_samples * np.array([n_hidden_units[0], n_hidden_units[1]])\n",
    "    n_x = m_samples * np.array([n_features, n_hidden_units[0],n_hidden_units[1]])\n",
    "    n_activation = np.append(n_m, *n_r)\n",
    "    n_theta_overhead = n_theta\n",
    "    n_network = sum(n_theta) + sum(n_phi) + sum(n_r) + sum(n_c) + n_m[1]\n",
    "    n_overhead = sum(n_theta) + sum(n_phi) + sum(n_m)\n",
    "    n_sampling = sum(n_rate) + sum(n_x) + sum(n_phi) + sum(n_activation) + sum(n_theta_overhead)\n",
    "    \n",
    "    return n_network, n_overhead, n_sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b472610f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network: 182271 parameters (1.46 MB)\n",
      "Overhead: 188264 parameters (1.51 MB)\n",
      "Sampling 90502270 parameters (724.02 MB)\n",
      "Total: 90872805 (0.73 GB)\n"
     ]
    }
   ],
   "source": [
    "n_network, n_overhead, n_sampling = network_size(*X_train.shape)\n",
    "\n",
    "# n_network, n_overhead, n_sampling = network_size(m_samples=1_000, n_features=13_000)\n",
    "# n_network, n_overhead = network_size(11314, 15593)\n",
    "n_total = n_network + n_overhead + n_sampling\n",
    "print('Network:', n_network, f'parameters ({n_network*8e-6:.2f} MB)')\n",
    "print('Overhead:', n_overhead, f'parameters ({n_overhead*8e-6:.2f} MB)')\n",
    "print('Sampling', n_sampling, f'parameters ({n_sampling*8e-6:.2f} MB)')\n",
    "n_GB = n_total * 8e-9\n",
    "print('Total:', n_total, f'({n_GB:.2f} GB)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22b1830c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@hk.transform_with_state\n",
    "def kernel(n_hidden_units = (200, )):\n",
    "    \"\"\"Advance the Markov chain by one step.\"\"\"\n",
    "    model = MultinomialDirichletBelieve(n_hidden_units, n_features)\n",
    "    # Do one Gibbs sampling step.\n",
    "    model(X_train)\n",
    "    \n",
    "@hk.without_apply_rng\n",
    "@hk.transform_with_state\n",
    "def _log_prob():\n",
    "    n_hidden_units = (int(n_features**0.25), int(n_features**0.5))\n",
    "    model = MultinomialDirichletBelieve(n_hidden_units, n_features)\n",
    "    return model.log_prob(X_train)\n",
    "\n",
    "log_prob = jax.vmap(_log_prob.apply, in_axes=[None, 0, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e75d9a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Number of trials is fixed at n_trials = [1000. 1001. 1002. 1003. 1004. 1005. 1006. 1007. 1008. 1009. 1010. 1011.\n",
      " 1012. 1013. 1014. 1015. 1016. 1017. 1018. 1019. 1020. 1021. 1022. 1023.\n",
      " 1024. 1025. 1026. 1027. 1028. 1029. 1030. 1031. 1032. 1033. 1034. 1035.\n",
      " 1036. 1037. 1038. 1039. 1040. 1041. 1042. 1043. 1044. 1045. 1046. 1047.\n",
      " 1048. 1049. 1050. 1051. 1052. 1053. 1054. 1055. 1056. 1057. 1058. 1059.\n",
      " 1060. 1061. 1062. 1063. 1064. 1065. 1066. 1067. 1068. 1069. 1070. 1071.\n",
      " 1072. 1073. 1074. 1075. 1076. 1077. 1078. 1079. 1080. 1081. 1082. 1083.\n",
      " 1084. 1085. 1086. 1087. 1088. 1089. 1090. 1091. 1092. 1093. 1094. 1095.\n",
      " 1096. 1097. 1098. 1099. 1100. 1101. 1102. 1103. 1104. 1105. 1106. 1107.\n",
      " 1108. 1109. 1110. 1111. 1112. 1113. 1114. 1115. 1116. 1117. 1118. 1119.\n",
      " 1120. 1121. 1122. 1123. 1124. 1125. 1126. 1127. 1128. 1129. 1130. 1131.\n",
      " 1132. 1133. 1134. 1135. 1136. 1137. 1138. 1139. 1140. 1141. 1142. 1143.\n",
      " 1144. 1145. 1146. 1147. 1148. 1149. 1150. 1151. 1152. 1153. 1154. 1155.\n",
      " 1156. 1157. 1158. 1159. 1160. 1161. 1162. 1163. 1164. 1165. 1166. 1167.\n",
      " 1168. 1169. 1170. 1171. 1172. 1173. 1174. 1175. 1176. 1177. 1178. 1179.\n",
      " 1180. 1181. 1182. 1183. 1184. 1185. 1186. 1187. 1188. 1189. 1190. 1191.\n",
      " 1192. 1193. 1194. 1195. 1196. 1197. 1198. 1199. 1200. 1201. 1202. 1203.\n",
      " 1204. 1205. 1206. 1207. 1208. 1209. 1210. 1211. 1212. 1213. 1214. 1215.\n",
      " 1216. 1217. 1218. 1219. 1220. 1221. 1222. 1223. 1224. 1225. 1226. 1227.\n",
      " 1228. 1229. 1230. 1231. 1232. 1233. 1234. 1235. 1236. 1237. 1238. 1239.\n",
      " 1240. 1241. 1242. 1243. 1244. 1245. 1246. 1247. 1248. 1249. 1250. 1251.\n",
      " 1252. 1253. 1254. 1255. 1256. 1257. 1258. 1259. 1260. 1261. 1262. 1263.\n",
      " 1264. 1265. 1266. 1267. 1268. 1269. 1270. 1271. 1272. 1273. 1274. 1275.\n",
      " 1276. 1277. 1278. 1279. 1280. 1281. 1282. 1283. 1284. 1285. 1286. 1287.\n",
      " 1288. 1289. 1290. 1291. 1292. 1293. 1294. 1295. 1296. 1297. 1298. 1299.\n",
      " 1300. 1301. 1302. 1303. 1304. 1305. 1306. 1307. 1308. 1309. 1310. 1311.\n",
      " 1312. 1313. 1314. 1315. 1316. 1317. 1318. 1319. 1320. 1321. 1322. 1323.\n",
      " 1324. 1325. 1326. 1327. 1328. 1329. 1330. 1331. 1332. 1333. 1334. 1335.\n",
      " 1336. 1337. 1338. 1339. 1340. 1341. 1342. 1343. 1344. 1345. 1346. 1347.\n",
      " 1348. 1349. 1350. 1351. 1352. 1353. 1354. 1355. 1356. 1357. 1358. 1359.\n",
      " 1360. 1361. 1362. 1363. 1364. 1365. 1366. 1367. 1368. 1369. 1370. 1371.\n",
      " 1372. 1373. 1374. 1375. 1376. 1377. 1378. 1379. 1380. 1381. 1382. 1383.\n",
      " 1384. 1385. 1386. 1387. 1388. 1389. 1390. 1391. 1392. 1393. 1394. 1395.\n",
      " 1396. 1397. 1398. 1399. 1400. 1401. 1402. 1403. 1404. 1405. 1406. 1407.\n",
      " 1408. 1409. 1410. 1411. 1412. 1413. 1414. 1415. 1416. 1417. 1418. 1419.\n",
      " 1420. 1421. 1422. 1423. 1424. 1425. 1426. 1427. 1428. 1429. 1430. 1431.\n",
      " 1432. 1433. 1434. 1435. 1436. 1437. 1438. 1439. 1440. 1441. 1442. 1443.\n",
      " 1444. 1445. 1446. 1447. 1448. 1449. 1450. 1451. 1452. 1453. 1454. 1455.\n",
      " 1456. 1457. 1458. 1459. 1460. 1461. 1462. 1463. 1464. 1465. 1466. 1467.\n",
      " 1468. 1469. 1470. 1471. 1472. 1473. 1474. 1475. 1476. 1477. 1478. 1479.\n",
      " 1480. 1481. 1482. 1483. 1484. 1485. 1486. 1487. 1488. 1489. 1490. 1491.\n",
      " 1492. 1493. 1494. 1495. 1496. 1497. 1498. 1499. 1500. 1501. 1502. 1503.\n",
      " 1504. 1505. 1506. 1507. 1508. 1509. 1510. 1511. 1512. 1513. 1514. 1515.\n",
      " 1516. 1517. 1518. 1519. 1520. 1521. 1522. 1523. 1524. 1525. 1526. 1527.\n",
      " 1528. 1529. 1530. 1531. 1532. 1533. 1534. 1535. 1536. 1537. 1538. 1539.\n",
      " 1540. 1541. 1542. 1543. 1544. 1545. 1546. 1547. 1548. 1549. 1550. 1551.\n",
      " 1552. 1553. 1554. 1555. 1556. 1557. 1558. 1559. 1560. 1561. 1562. 1563.\n",
      " 1564. 1565. 1566. 1567. 1568. 1569. 1570. 1571. 1572. 1573. 1574. 1575.\n",
      " 1576. 1577. 1578. 1579. 1580. 1581. 1582. 1583. 1584. 1585. 1586. 1587.\n",
      " 1588. 1589. 1590. 1591. 1592. 1593. 1594. 1595. 1596. 1597. 1598. 1599.\n",
      " 1600. 1601. 1602. 1603. 1604. 1605. 1606. 1607. 1608. 1609. 1610. 1611.\n",
      " 1612. 1613. 1614. 1615. 1616. 1617. 1618. 1619. 1620. 1621. 1622. 1623.\n",
      " 1624. 1625. 1626. 1627. 1628. 1629. 1630. 1631. 1632. 1633. 1634. 1635.\n",
      " 1636. 1637. 1638. 1639. 1640. 1641. 1642. 1643. 1644. 1645. 1646. 1647.\n",
      " 1648. 1649. 1650. 1651. 1652. 1653. 1654. 1655. 1656. 1657. 1658. 1659.\n",
      " 1660. 1661. 1662. 1663. 1664. 1665. 1666. 1667. 1668. 1669. 1670. 1671.\n",
      " 1672. 1673. 1674. 1675. 1676. 1677. 1678. 1679. 1680. 1681. 1682. 1683.\n",
      " 1684. 1685. 1686. 1687. 1688. 1689. 1690. 1691. 1692. 1693. 1694. 1695.\n",
      " 1696. 1697. 1698. 1699. 1700. 1701. 1702. 1703. 1704. 1705. 1706. 1707.\n",
      " 1708. 1709. 1710. 1711. 1712. 1713. 1714. 1715. 1716. 1717. 1718. 1719.\n",
      " 1720. 1721. 1722. 1723. 1724. 1725. 1726. 1727. 1728. 1729. 1730. 1731.\n",
      " 1732. 1733. 1734. 1735. 1736. 1737. 1738. 1739. 1740. 1741. 1742. 1743.\n",
      " 1744. 1745. 1746. 1747. 1748. 1749. 1750. 1751. 1752. 1753. 1754. 1755.\n",
      " 1756. 1757. 1758. 1759. 1760. 1761. 1762. 1763. 1764. 1765. 1766. 1767.\n",
      " 1768. 1769. 1770. 1771. 1772. 1773. 1774. 1775. 1776. 1777. 1778. 1779.\n",
      " 1780. 1781. 1782. 1783. 1784. 1785. 1786. 1787. 1788. 1789. 1790. 1791.\n",
      " 1792. 1793. 1794. 1795. 1796. 1797. 1798. 1799. 1800. 1801. 1802. 1803.\n",
      " 1804. 1805. 1806. 1807. 1808. 1809. 1810. 1811. 1812. 1813. 1814. 1815.\n",
      " 1816. 1817. 1818. 1819. 1820. 1821. 1822. 1823. 1824. 1825. 1826. 1827.\n",
      " 1828. 1829. 1830. 1831. 1832. 1833. 1834. 1835. 1836. 1837. 1838. 1839.\n",
      " 1840. 1841. 1842. 1843. 1844. 1845. 1846. 1847. 1848. 1849. 1850. 1851.\n",
      " 1852. 1853. 1854. 1855. 1856. 1857. 1858. 1859. 1860. 1861. 1862. 1863.\n",
      " 1864. 1865. 1866. 1867. 1868. 1869. 1870. 1871. 1872. 1873. 1874. 1875.\n",
      " 1876. 1877. 1878. 1879. 1880. 1881. 1882. 1883. 1884. 1885. 1886. 1887.\n",
      " 1888. 1889. 1890. 1891. 1892. 1893. 1894. 1895. 1896. 1897. 1898. 1899.\n",
      " 1900. 1901. 1902. 1903. 1904. 1905. 1906. 1907. 1908. 1909. 1910. 1911.\n",
      " 1912. 1913. 1914. 1915. 1916. 1917. 1918. 1919. 1920. 1921. 1922. 1923.\n",
      " 1924. 1925. 1926. 1927. 1928. 1929. 1930. 1931. 1932. 1933. 1934. 1935.\n",
      " 1936. 1937. 1938. 1939. 1940. 1941. 1942. 1943. 1944. 1945. 1946. 1947.\n",
      " 1948. 1949. 1950. 1951. 1952. 1953. 1954. 1955. 1956. 1957. 1958. 1959.\n",
      " 1960. 1961. 1962. 1963. 1964. 1965. 1966. 1967. 1968. 1969. 1970. 1971.\n",
      " 1972. 1973. 1974. 1975. 1976. 1977. 1978. 1979. 1980. 1981. 1982. 1983.\n",
      " 1984. 1985. 1986. 1987. 1988. 1989. 1990. 1991. 1992. 1993. 1994. 1995.\n",
      " 1996. 1997. 1998. 1999.]\n",
      "2023-07-17 19:42:28.978209: E external/xla/xla/service/slow_operation_alarm.cc:65] Constant folding an instruction is taking > 1s:\n",
      "\n",
      "  %reduce-window.89 = f32[1000,63]{1,0} reduce-window(f32[1000,2000]{1,0} %constant.8595, f32[] %constant.8592), window={size=1x32 stride=1x32 pad=0_0x8_8}, to_apply=%region_513.25844\n",
      "\n",
      "This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime. XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.\n",
      "\n",
      "If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.\n",
      "2023-07-17 19:42:29.294603: E external/xla/xla/service/slow_operation_alarm.cc:133] The operation took 1.323703907s\n",
      "Constant folding an instruction is taking > 1s:\n",
      "\n",
      "  %reduce-window.89 = f32[1000,63]{1,0} reduce-window(f32[1000,2000]{1,0} %constant.8595, f32[] %constant.8592), window={size=1x32 stride=1x32 pad=0_0x8_8}, to_apply=%region_513.25844\n",
      "\n",
      "This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime. XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.\n",
      "\n",
      "If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.\n"
     ]
    }
   ],
   "source": [
    "params, states = sample_markov_chain(\n",
    "    next(key_seq), \n",
    "    kernel=kernel, \n",
    "    n_samples=20, \n",
    "    n_burnin_steps=20,\n",
    "    n_chains=jax.device_count(),\n",
    ")\n",
    "_ = states['multinomial_dirichlet_believe/~/cap_layer']['r'].block_until_ready()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44c376cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint i=0.\n"
     ]
    }
   ],
   "source": [
    "last_state = save_last_state(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92865192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint i=1.\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    params, states = sample_markov_chain(\n",
    "        next(key_seq), \n",
    "        kernel=kernel,\n",
    "        n_samples=80, \n",
    "        n_burnin_steps=0, \n",
    "        initial_state=last_state,\n",
    "        params=params,\n",
    "    )\n",
    "    last_state = save_last_state(states)\n",
    "    lls, _ = log_prob(params, last_state)\n",
    "    print('Log likelihood', lls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebed782",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
