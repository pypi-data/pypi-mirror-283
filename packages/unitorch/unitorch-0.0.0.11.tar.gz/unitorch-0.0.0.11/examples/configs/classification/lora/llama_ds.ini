[core/cli]
task_name = core/task/deepspeed/supervised
from_ckpt_dir = ./cache
cache_dir = ./cache
train_file = ./train.tsv
dev_file = ./dev.tsv
test_file = ./test.tsv
pretrained_weight_folder = ./llama-7b

# model
[core/model/classification/peft/lora/llama]
pretrained_name = llama-7b
pretrained_weight_path = [
    '${core/cli:pretrained_weight_folder}/pytorch_model-00001-of-00002.bin',
    '${core/cli:pretrained_weight_folder}/pytorch_model-00002-of-00002.bin',
  ]
num_classes = 2

# dataset
[core/dataset/ast]
names = ['query', 'doc', 'label']

[core/dataset/ast/train]
data_files = ${core/cli:train_file}
preprocess_functions = ['core/process/llama/classification(query, doc)', 'core/process/label(label)']

[core/dataset/ast/dev]
data_files = ${core/cli:dev_file}
preprocess_functions = ['core/process/llama/classification(query, doc)', 'core/process/label(label)']

[core/dataset/ast/test]
data_files = ${core/cli:test_file}
preprocess_functions = ['core/process/llama/classification(query, doc)']

# process
[core/process/llama]
pretrained_name = llama-7b
max_seq_length = 6
max_gen_seq_length = 36

[core/writer/csv]
escapechar = \

# task
[core/task/deepspeed/supervised]
config_path = https://raw.githubusercontent.com/fuliucansheng/unitorch/master/examples/configs/deepspeed/adamw.json
model = core/model/classification/peft/lora/llama
dataset = core/dataset/ast
loss_fn = core/loss/ce
score_fn = core/score/acc
monitor_fns = ['core/score/acc']

ckpt_freq = 2000
learning_rate = 0.00001

output_header = ['query', 'doc', 'label']
postprocess_fn = core/postprocess/classification/score
writer = core/writer/csv

from_ckpt_dir = ${core/cli:from_ckpt_dir}
to_ckpt_dir = ${core/cli:cache_dir}
output_path = ${core/cli:cache_dir}/output.txt
train_batch_size = 4
dev_batch_size = 4
test_batch_size = 4
